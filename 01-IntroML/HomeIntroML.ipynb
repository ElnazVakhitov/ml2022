{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport pandas as pd\nimport sklearn\nfrom numpy.testing import assert_array_almost_equal, assert_equal, assert_almost_equal\n\nimport warnings\nwarnings.filterwarnings('ignore')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Первое обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простое как пробка задание. Обучите классификатор [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) на входных данных с гиперпараметрами:\n\n* `max_depth`=$6$\n* `min_samples_split`=$3$\n* `min_samples_leaf`=$3$\n* `n_estimators`=$100$\n* `n_jobs`=$-1$\n\nИ верните обученную модель.\n\nДанные в X только численные, в y только 2 значения: 0 и 1. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef fit_rf(X: np.ndarray, y:np.ndarray) ->  RandomForestClassifier:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\nnp.random.seed(1337)\nn = 200\na = np.random.normal(loc=0, scale=1, size=(n, 2)) # первый класс\nb = np.random.normal(loc=3, scale=2, size=(n, 2)) # второй класс\nX_clf = np.vstack([a, b]) # двумерный количественный признак\ny_clf = np.hstack([np.zeros(n), np.ones(n)]) # бинарный признак\n\nmodel = fit_rf(X_clf, y_clf)\nassert model.n_estimators == 100\nassert model.max_depth == 6\nassert model.min_samples_split == 3\nassert model.min_samples_leaf == 3\n\nassert_equal(model.predict(np.array([[0, 0]])), np.array([0.]))\nassert_equal(model.predict(np.array([[3, 3]])), np.array([1.]))\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Первая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В [папке data](https://github.com/samstikhin/ml2021/tree/master/02-IntroML) вы можете найти данные для бинарной классификации (`diabets_train.csv`, `diabets_test.csv` и `diabetes_answers.csv`). $Y$ в этих данных выступает столбик `Outcome`, в качестве $X$ - все остальное. \n\nВам необходимо предсказать $y_{test}$ такой, что $accuracy > 0.75$ ([доля правильных ответов](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)). Вы можете делать что угодно, чтобы получить результат:\n\n* использовать любой классификатор с любыми гиперпараметрами\n* как угодно изменять данные \n\nВернуть в этом случае нужно не модель, а результат - одномерный массив данных $y_{pred}$ (предсказание $y_{test}$).\n\nP.S. Можете узнать больше о данных по [ссылке](https://www.kaggle.com/uciml/pima-indians-diabetes-database). Мы произвольным образом разбили данные в соотношении 4:1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n\ndef classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\ndf_train = pd.read_csv('data/diabetes_train.csv')\ndf_test = pd.read_csv('data/diabetes_test.csv')\n\nX_train = df_train.drop(columns=['Outcome']).values\ny_train = df_train['Outcome']\n\nX_test = df_test.values\ny_test = pd.read_csv('data/diabetes_answers.csv')['Outcome']\n\ny_pred = classification(X_train, y_train, X_test)\nassert sklearn.metrics.accuracy_score(y_test, y_pred) > 0.74\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Переобучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В [папке data](https://github.com/samstikhin/ml2021/tree/master/02-IntroML) вы можете найти данные для бинарной классификации (файлы `overfit_trian.csv`, `overfit_test.csv`). Вам на вход подается тренировочная и тестовая выборки из файла. \n\nВерните такую обученную модель, которая на тренировочной выборке дает $accuracy > 0.97$, а на тестовом $accuracy < 0.7$.\n\n[$accuracy$](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) - доля правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n\ndef overfitting(X_train: np.array, y_train: np.array, X_test: np.array, y_test: np.array):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\ndf_train = pd.read_csv('data/overfit_train.csv')\ndf_test = pd.read_csv('data/overfit_test.csv')\n\nX_train = df_train.drop(columns=['y']).values\ny_train = df_train['y']\n\nX_test = df_test.drop(columns=['y']).values\ny_test = df_test['y']\n\nmodel = overfitting(X_train, y_train, X_test, y_test)\n\ny_train_pred = model.predict(X_train)\ny_test_pred =  model.predict(X_test)\nassert sklearn.metrics.accuracy_score(y_train, y_train_pred) > 0.97\nassert sklearn.metrics.accuracy_score(y_test, y_test_pred) < 0.7\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мой KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача реализовать свой простой KNNClassifier для бинарных данных. Вам нужно реализовать 3 метода:\n\n* `init` - начальная инициализация\n* `fit` - обучение классификатора\n* `predict` - предсказание для новых объектов\n* `predict_proba` - предсказание вероятностей новых объектов\n\nУ нашего классификатора будет лишь один гиперпараметр - количество соседей $k$. Во избежании тонкостей: $k$ - нечетное.\n\nНа вход будет подаваться выборка объектов $X$, у которых ровно 2 числовых признака. $y$ - результат бинарной классификации $0$ или $1$.\n\nМетрика ближайших элементов - Эвклидова.\n\nНапоминание: $y$ - одномерный массив, $X$ - двумерный массив, по $0$-ой оси которой расположены объекты.\n\n### Sample 1\n#### Input:\n```python\nX_train = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny_train = np.array([1, 1, 0, 0])\n\nmodel = KNN(k=3).fit(X_train, y_train)\ny_pred = model.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]]))\ny_prob = model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n```\n#### Output:\n```python\ny_pred = np.array([1., 0.])\ny_prob = np.array([[0.33, 0.667],\n                   [0.667, 0.33]])\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class KNN():\n    def __init__(self, k=3):\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass\n\n    def fit(self, X_train: np.array, y_train:np.array): #обучаем классификатор\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        return self\n\n    def predict(self, X_test: np.array): #предсказываем значения\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass\n    \n    def predict_proba(self, X_test: np.array): #предсказываем вероятности\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n######################################################\nX_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny_clf = np.array([1, 1, 0, 0])\n\nmodel = KNN(k=3).fit(X_clf, y_clf)\n\nassert_equal(model.predict(np.array([[-0.5, -0.5]])), np.array([0.]))\nassert_equal(model.predict(np.array([[ 0.5,  0.5]])), np.array([1.]))\nassert_almost_equal(model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]])), \n                    np.array([[0.33, 0.667],\n                              [0.667, 0.33]]), \n                    decimal=2)\n######################################################\nnp.random.seed(1337)\nn = 200\na = np.random.normal(loc=0, scale=1, size=(n, 2)) # первый класс\nb = np.random.normal(loc=3, scale=2, size=(n, 2)) # второй класс\nX = np.vstack([a, b]) # двумерный количественный признак\ny = np.hstack([np.zeros(n), np.ones(n)]) # бинарный признак\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n\nmodel = KNN(k=3).fit(X_train, y_train)\nmodel_real = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n\nassert_array_almost_equal(model.predict(X_test), model_real.predict(X_test))\nassert_array_almost_equal(model.predict_proba(X_test), model_real.predict_proba(X_test))\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Моя Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вам предстоит реализовать свою простейшую линейную регрессию по функционалу $MSE$.\n\nЛинейная регрессия выглядит следующим образом:\n$$a(x) = w_1x + w_0$$\n\nНеобходимо найти такие $w_0$ и $w_1$, при которых минимизируется значение\n\n$$MSE(X,Y) = \\frac{1}{n}\\sum_{i=1}^{n}(a(x_i) - y_i)^2$$\n\nВыведите формулы для $w_0$ и $w_1$ аналитически и реализуйте следующие методы класса \n\n* `init` - начальная инициализация\n* `fit` - обучение классификатора\n* `predict` - предсказание для новых объектов\n\nПосле обучения у модели должен присутствовать атрибут `model.coef_` из которого можно получить коэффициенты регрессии в порядке: $w_1$, $w_0$.\n\nГиперпараметры отсутствуют.\n\nНа вход будут подаваться два массива $X\\in \\mathbb{R}^{n}$ и $Y \\in \\mathbb{R}^{n}$.\n\nМетрика - Евклидова.\n\n### Sample 1\n#### Input:\n```python\nX_train = np.array([[1], [2]])\ny_train = np.array([1, 2])\n\nmodel = LinReg().fit(X_train, y_train)\ny_pred = model.predict(np.array([[3],[4]]))\n\n```\n#### Output:\n```python\ny_pred = np.array([3, 4])\nmodel.coef_ = np.array([1., 0.])\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class LinReg():\n    def __init__(self):\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass\n\n    def fit(self, X_train: np.array, y_train:np.array): #обучаем регрессию\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        return self\n\n    def predict(self, X_test: np.array): #предсказываем значения\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\nX_reg = np.array([[1], [2]])\ny_reg = np.array([1, 2])\n\nmodel = LinReg().fit(X_reg, y_reg)\nassert_array_almost_equal(model.predict(np.array([[3],[4]])), np.array([3, 4]), decimal=2)\n\nassert_array_almost_equal(model.predict(np.array([[0]])), np.array([0]), decimal=2)\n\nassert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=2)\n######################################################\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n\nX_reg, y_reg = make_regression(n_samples=200, n_features=1, n_targets=1)\n\nmodel = LinearRegression().fit(X_reg, y_reg)\nmodel2 = LinReg().fit(X_reg, y_reg)\n\ncoef_real = np.array([model.coef_[0], model.predict(np.array([[0]]))[0]])\ncoef_my = model2.coef_\n\nassert_array_almost_equal(coef_my, coef_real, decimal=3)\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Наивный (зато свой) Байес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требуется написать свой классификатор, на основе наивного баеса. Необходимо реализовать аналог `MultinomialNB`. \n\n$$y_{test} = argmax_{label}\\ln{P(y=label)} + \\sum_{i=1}^{m}\\ln{(P(x_i=value|y = label) + \\alpha)}, ~~~ label\\in\\{0,1\\}$$\n\nНа вход подаются численные категориальные признаки. Классы: $0$ и $1$. У классификатора будет единственный параметр - $alpha$.\n\nВспомогательные поля для упращения решения задачи:\nПоле `apriori` - это словарь со значениями априорных вероятностей: $P(y = 0)$ и $P(y = 1)$.\n\nПоле `posterior` - это словарь со значениями условных (апостериорных) вероятностей: $posterior[(label, i, value)] = P(x_i=value|y = label)$.\n\n* `label` - значение y\n* `i` - номер фичи в том порядке как и во входном `X_clf`\n* `value` - значение фичи\n\n\n### Sample 1:\n#### Input:\n```python\nX_clf = np.array([[10, 20], [10, 30], [20,20], [20, 30], [20, 40], [20, 40]])\ny_clf = np.array([1, 1, 0, 0, 0, 0])\n\nmodel = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n\n\ny_pred = model.predict(np.array([[10, 20], [20, 60]]))\n```\n#### Output:\n```python\ny_pred = [1, 0]\n\nmodel.apriori = {0: 0.666666, 1: 0.333333}\n\nmodel.posterior = {(1, 0, 10): 1.0, (1, 1, 20): 0.5,  (1, 1, 30): 0.5,\n                   (0, 0, 20): 1.0, (0, 1, 20): 0.25, (0, 1, 30): 0.25, (0, 1, 40): 0.5}\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from collections import defaultdict\nfrom math import log\n\nclass MyNaiveBayes():\n    def __init__(self, alpha=0.01):\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        self.alpha = alpha\n        self.apriori = {0: 0, 1:0}\n        self.posterior = defaultdict(lambda: 0)\n        pass\n        \n    def fit(self, X: np.ndarray, y: np.ndarray):\n        assert X.ndim == 2\n        assert y.ndim == 1\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        return self\n            \n    def predict(self, X: np.ndarray):\n        assert X.ndim == 2\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\nX_clf = np.array([[10, 20], [10, 30], [20,20], [20, 30], [20, 40], [20, 40]])\ny_clf = np.array([1, 1, 0, 0, 0, 0])\n\nmodel = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n\nassert np.abs(model.apriori[0] - 0.666) < 0.01\nassert np.abs(model.apriori[1] - 0.333) < 0.01\nassert model.posterior[(1, 0, 10)] == 1.0\nassert model.posterior[(1, 1, 30)] == 0.5\nassert model.posterior[(0, 0, 20)] == 1.0\nassert model.posterior[(0, 1, 20)] == 0.25\nassert model.posterior[(0, 1, 40)] == 0.5\n\nassert_equal(model.predict(np.array([[10, 20], [20, 60]])), np.array([1, 0]))\n######################################################\nX_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny_clf = np.array([1, 1, 0, 0])\n\nmodel = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n\nassert np.abs(model.apriori[0] - 0.5) < 0.01\nassert np.abs(model.apriori[1] - 0.5) < 0.01\nassert model.posterior[(0,1,1)]== 0.5\nassert model.posterior[(1,0,1)]== 1.0\n\n\nassert_equal(model.predict(np.array([[1, 2], [-1, -2]])), np.array([1, 0]))\n\n######################################################\nX_clf = np.array([[2], [2], [4]])\ny_clf = np.array([0, 0, 1])\n\nmodel = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n\n\nassert np.abs(model.apriori[0] - 0.666) < 0.01\nassert np.abs(model.apriori[1] - 0.333) < 0.01\nassert model.posterior[(0,0,2)] == 1.0\nassert model.posterior[(1,0,4)] == 1.0\n\n\n\nassert_equal(model.predict(np.array([[2], [4]])), np.array([0, 1]))\n######################################################\nX_clf = np.array([[4], [4], [4], [4], [2], [2], [2], [2], [2]])\ny_clf = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1])\n\nmodel = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n\nassert np.abs(model.apriori[0] - 0.777) < 0.01\nassert np.abs(model.apriori[1] - 0.222) < 0.01\nassert np.abs(model.posterior[(0,0,4)] - 0.5714) < 0.01\nassert np.abs(model.posterior[(0,0,2)] - 0.4285) < 0.01\nassert model.posterior[(1,0,2)] == 1.0\n\nassert_equal(model.predict(np.array([[2], [4]])), np.array([0, 0]))\n######################################################\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.naive_bayes import MultinomialNB\n\nrandom_state = 1448\nX, y = make_classification(n_samples=500, n_features=4, n_informative=2,\n                           scale=5, random_state=random_state)\n\nX = X.astype(np.int) \nX += np.abs(np.min(X))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=random_state,\n                                                    stratify=y)\n\nclf = MyNaiveBayes(alpha=0.01).fit(X_train, y_train)\nassert accuracy_score(y_test, clf.predict(X_test)) > 0.9\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Злобные твиты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивный Байес очень хорошо подходит для Анализа тональности текста. То есть определить несет текст позитивный или негативный вайб :) \n\nВ данной задаче вам даны [твиты про разные авиакомпании](https://github.com/samstikhin/ml2021/tree/master/02-IntroML) (файлы `tweets_train.csv`, `tweets_test.csv`). Требуется построить классификатор, который бы определял имеет ли твит положительную или негативную окраску. Напишите функцию `predict`, который возвращает массив, где 1 - это позитивный, 0 - негативный. \n\nВ данной задаче вам понадобятся `TweetTokenizer` и `CountVectorizer`. \n\n* `TweetTokenizer` и любой tokenizer самостоятельно разбивает строку на слова как-то их модифицировав.\n* `CountVectorizer` превращает предложение в вектор чисел основываясь на их частоте, используя токенайзер.\n\nЗадача делится на 2 пункта: \n\n* Preprocessing: переведите все слова в нижний регистр, токенизируйте слова с помощью `TweetTokenizer`, а дальше опять соберите в предложение. Проделайте это и в $X_{train}$ и $X_{test}$\n* Predict: переведите предложения в вектора количеств с помощь `CountVectorizer`. Обучите модель на данных и предскажите ответ.\n\nЧтобы получить баллы надо побить accuracy = 0.88\n\n\nP.S Можете посмотреть топ слов, которые с наибольшей вероятностью относятся к позитивным и негативным классам. \n\nP.P.S. Если ничего не понятно: можете изучить различные kernel на [kaggle](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, TweetTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ndef predict(df_train: pd.DataFrame, df_test: pd.DataFrame):\n    \n    # preprocess data here\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    model = MultinomialNB ...\n        \n    # fit model\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    return model.predict(...) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\nfrom sklearn.metrics import accuracy_score\n\ndf_train = pd.read_csv('data/tweets_train.csv')\ndf_test = pd.read_csv('data/tweets_test.csv')\n\ny_test = ((df_test['airline_sentiment'] == 'positive').astype(np.int))\nX_test = df_test.drop(columns='airline_sentiment')\n\nassert accuracy_score(y_test, predict(df_train, df_test)) > 0.88\n######################################################"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}