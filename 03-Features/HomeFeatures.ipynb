{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nimport pandas as pd\nimport sklearn\nimport sklearn.metrics\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\nfrom pandas.testing import assert_frame_equal\nfrom typing import Tuple\n\nimport warnings\nwarnings.filterwarnings('ignore')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основы метрик классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подаются 2 массива: \n\n* $y_{real}$ - реальные значения бинарных классов\n* $y_{pred}$ - предсказанные значения бинарных классов. \n\nВам необходимо посчитать, **не используя** стандартные функции, метрики: \n\n* $accuracy$\n* $precision$\n* $recall$\n* $F_1$\n\nВозвращать числа нужно именно в данном порядке.\n\n### Sample 1\n#### Input:\n```python\ny_real = np.array([0, 1, 0, 0, 1, 1, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 1, 1, 0, 0])\n```\n#### Output:\n```python\n0.625, 0.75, 0.6, 0.66\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n\ndef main_metrics(y_real: np.array, y_pred: np.array) -> (float, float, float, float):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\ny_real = np.array([0, 1, 0, 0, 1, 1, 1, 1])\ny_pred = np.array([0, 1, 1, 0, 1, 1, 0, 0])\n\nacc, pre, rec, f1 = main_metrics(y_real, y_pred)\n\nassert np.abs(acc - sklearn.metrics.accuracy_score(y_real, y_pred)) < 0.001\nassert np.abs(pre - sklearn.metrics.precision_score(y_real, y_pred)) < 0.001\nassert np.abs(rec - sklearn.metrics.recall_score(y_real, y_pred)) < 0.001\nassert np.abs(f1  - sklearn.metrics.f1_score(y_real, y_pred)) < 0.001\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основы метрик регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решаем задачу регрессии. На вход подаются 2 массива: $y_{real}$ - реальные значения функции и $y_{pred}$ - предсказанные значения функции. \n\nВам необходимо посчитать, **не используя** стандартные функции, метрики: \n\n* $R^2score$\n* $MAE$ - `mean_absolute_error`\n* $MSE$ - `mean_squared_error`\n* $MSLE$ - `mean_squared_log_error`\n\nВозвращать числа нужно именно в данном порядке.\n\nМожете сверяться с реальными метриками в `sklearn.metrics`.\n\nВсе числа в тестах больше 0, поэтому $MSLE$ будет считаться корректно.\n### Sample 1\n#### Input:\n```python\ny_real = np.array([1, 2, 3, 4, 6])\ny_pred = np.array([1, 3, 2, 4, 5])\n```\n#### Output:\n```python\n0.797297, 0.6, 0.6, 0.037856\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n\ndef reg_metrics(y_real: np.array, y_pred: np.array) -> (float, float, float, float):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\ny_real = np.array([1,2,3,4,6])\ny_pred = np.array([1,3,2,4,5])\n\nr2, mae, mse, msle = reg_metrics(y_real, y_pred)\n\nassert np.abs(r2 - sklearn.metrics.r2_score(y_real, y_pred)) < 0.001\nassert np.abs(mae - sklearn.metrics.mean_absolute_error(y_real, y_pred)) < 0.001\nassert np.abs(mse - sklearn.metrics.mean_squared_error(y_real, y_pred)) < 0.001\nassert np.abs(msle  - sklearn.metrics.mean_squared_log_error(y_real, y_pred)) < 0.001\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нахождение Roc-curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам на вход даны $y_{real}$ и массив вероятностей $y_{prob} = P(y_{pred}=1)$ необходимо реализовать функцию `roc-curve`, которая вернет 2 массива различных значений $fpr$ и $tpr$, для дальнейшего построения $Roc$ кривой.\n\nМожно считать, что все вероятности ограничены $decimal=2$ (у каждого числа не более 2-х знаков после запятой).\n\n### Sample\n#### Input:\n```python\ny_real = np.array([  1,   1,   0,   0,   0,   1,   0,   1,   0])\ny_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6, 0.4])\n```\n#### Output:\n```python\narray([0.,  0.,  0.4, 0.6, 1. ]), #fpr\narray([0., 0.5, 0.75,  1., 1. ])  #tpr\n```\n\n### Sample 2\n#### Input:\n```python\ny_real = np.array([  1,   1,   0,   0,   1,   0,   1,   0])\ny_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.4, 0.4, 0.6, 0.6])\n```\n#### Output:\n```python\narray([0.,  0., 0.25, 0.5, 1. ]), #fpr\narray([0., 0.5, 0.75,  1., 1. ])  #tpr\n\nили \n\narray([0.,  0., 0.5, 1. ]), #fpr\narray([0., 0.5,  1., 1. ])  #tpr\n```\n\nОбратите внимание на 2 пример: roc кривая, которая задается ими - одинаковая. Точка, которая уходит, находится на прямой между двумя соседними, в целом такие точки можно убирать, но будут приниматься оба варианта. Функция `sklearn.metrics.roc_curve` возвращает второй вариант."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n\ndef roc(y_real: np.array, y_prob: np.array) -> (np.array, np.array):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import auc\n######################################################\ny_real = np.array([  1,   1,   0,   0,   0,   1,   0,   1,   0])\ny_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.6, 0.4, 0.6, 0.6, 0.4])\n\nfpr, tpr = roc(y_real, y_prob)\nfpr_true, tpr_true, _ = sklearn.metrics.roc_curve(y_real, y_prob)\n\nassert np.abs(auc(fpr, tpr) - auc(fpr_true, tpr_true)) < 0.01\n######################################################\ny_real = np.array([  1,   1,   0,   0,   1,   0,   1,   0])\ny_prob = np.array([0.8, 0.8, 0.2, 0.2, 0.4, 0.4, 0.6, 0.6])\n\nfpr, tpr = roc(y_real, y_prob)\nfpr_true, tpr_true, _  = sklearn.metrics.roc_curve(y_real, y_prob)\n\nassert np.abs(auc(fpr, tpr) - auc(fpr_true, tpr_true)) < 0.01\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C помощью [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) найдите лучшие коэффициенты гиперпараметров `max_depth` и `min_samples_leaf` для классификатора [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) и верните обученный grid_search. \n\n* Пределы `max_depth` $(1, 10)$ \n* Пределы `min_samples_leaf` $(1, 10)$  \n* Входные данные в `data/sonar.csv`\n* scoring - `precision`\n* cv - $5$\n* Другие параметры в `DecisionTreeClassifier` не указывать.\n\nНе нужно Shuffl-ить данные, это может повлиять на ответ и в итоге задача не зачтется."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\nfrom sklearn.model_selection import GridSearchCV\n\ndef fit_gs(X: np.ndarray, y: np.ndarray) ->  GridSearchCV:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# В этом задании нет открытых тестов ;)\n\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Удаление Nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Серия задач в данном модуле объединена в одну [большую задачу по предсказанию данных](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview). В ходе выполнения модуля мы будем разбирать определенные техники, которые нужны для ее решения. Настоятельно рекомендуем выполнить все шаги **по порядку**, тогда в конце вы получите решение большой реальной задачи по МЛ.\n\nНам даны [данные](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) о домах выставленных на продажу. Нам необходимо решить задачу регрессии и предсказать цену продажи дома для $X_{test}$ по данным $X_{train}$ и $y_{train}$. В нашем случае  $y_{train}$ - это столбик `SalePrice`, $X_{train}$ - все остальные столбики.\n\nНа вход подается 2 считанных датафрейма **df_train**, **df_test** из файлов без изменений. \n\nНачальная подготовка:\n\n* Разделить **df_train** на **X_train**(`pd.Dataframe`) и **y_train**(`pd.Series`).\n* Сконкатенировать **X_train** и **df_test** в **df** по вертикали (можно ориентироваться по столбику `Id` они как раз идут по-порядку). Не забудьте обновить индекс!\n\nЗадачи:\n\n* Заменить в **df** все Nan-ы в категориальных признаках (`object`) на строку `missing`\n* Заменить в **df** все Nan-ы в числовых признаках на 0.\n\nВернуть из функции измененный **df**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n\ndef del_nan(df_train: pd.DataFrame, df_test: pd.DataFrame) -> pd.DataFrame:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\ntrain = pd.read_csv(\"data/train.csv\")\ntest = pd.read_csv(\"data/test.csv\")\ndel_nan_df = del_nan(train, test)\n\nassert_frame_equal(del_nan_df, pd.read_csv('data/del_nan.csv'))\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Порядковые категории"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам на вход приходит **df** из предыдущей задачи.\n\nЕсли внимательно изучить файл `data_description` можно понять, что многие категориальные признаки - порядковые (упорядоченное множество). Значит их можно перевести в осмысленные числа. Значит тут можно воспользоваться `LabelEncoding`.\n\nВаша задача: заменить в **df** категориальные признаки на числовые, для порядковых признаков.\n\nНа выходе возвращаем измененный **df**.\n\nЧтобы слегка упростить вам жизнь, вот вам готовые словари для перевода. Однако к каким столбцам их применять - вы должны выяснить сами, изучив файл `data_description`. Каждый маппинг используется хотя бы 1 раз, а некоторые и не по одному разу.\n\n```python\n{'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'missing':0}\n{'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'missing':0}\n{'Gd':4, 'Av': 3, 'Mn': 2, 'No': 1, 'missing': 0}\n{'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'missing': 0}\n{'Typ': 8, 'Min1': 7, 'Min2': 6, 'Mod': 5, 'Maj1': 4, 'Maj2': 3, 'Sev': 2, 'Sal': 1, 'missing': 0}\n{'Fin': 3, 'RFn': 2, 'Unf': 1, 'missing': 0}\n{'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'missing': 0}\n{'Reg': 4, 'IR1': 3, 'IR2': 2, 'IR3': 1, 'missing': 0}\n{'Lvl': 4, 'Bnk': 3, 'HLS':2,'Low':1, 'missing': 0}\n{'AllPub':4, 'NoSewr':3, 'NoSeWa':2, 'ELO':1, 'missing':0}\n{'Gtl':3, 'Mod':2, 'Sev':1, 'missing':0}\n{'SBrkr':5, 'FuseA':4, 'FuseF':3, 'FuseP':2, 'Mix':1, 'missing':0}\n{'Y':3, 'P':2, 'N':1, 'missing':0}\n{'Y':1, 'N':0, 'missing':0} #тут нет ошибки, все так и задумано:)\n```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n\ndef cat_to_num(df: pd.DataFrame) -> pd.DataFrame:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\ndel_nan_df = pd.read_csv('data/del_nan.csv')\ncat_to_num_df = cat_to_num(del_nan_df)\n\nassert_frame_equal(cat_to_num_df, pd.read_csv('data/cat_to_num.csv'))\n\ncategorical_cols = [col for col in cat_to_num_df.columns if cat_to_num_df[col].dtypes == \"object\"]\nassert len(categorical_cols) == 20\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разберемся с непорядковыми категориальными признаками.\n\nДля начала заметим признак `MSSubClass`, у которого тип `int64`, но если посмотреть в описание `data_description` можно понять, что это - категориальный признак. \n\n* Измените тип признака `MSSubClass` с `int64` на `object`\n\nТеперь можно сделать `One hot encoding`:\n\n* Найдите все колонки с категориальными признаками и составьте из них отдельный **df_oh** `pd.DataFrame` (индекс сохранить прежний)\n* Применить к полученному фрейму **df_oh** функцию [`pd.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) (Реализует `One Hot Encoding`)\n* Удалить категориальные колонки из **df** и добавить справа к **df** фрейм с `One Hot Encoding`\n \nВернуть из функции **df**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n\ndef one_hot(df: pd.DataFrame) -> pd.DataFrame:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# В этой задаче нет открытых тестов ;)\n\n######################################################\ncat_to_num_df = pd.read_csv('data/cat_to_num.csv')\none_hot_df = one_hot(cat_to_num_df)\none_hot_ans = pd.read_csv('data/one_hot.csv')\n\nassert_frame_equal(one_hot_df.astype('float64').reindex(sorted(one_hot_df.columns), axis=1), \n                    one_hot_ans.astype('float64').reindex(sorted(one_hot_ans.columns), axis=1))\n\nassert one_hot_df.shape[1] == 238\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Некоррелирующие признаки "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы разобрались с категориальными признаками, теперь разберемся с числовыми.\n\nДля числовых признаков можно посчитать корреляцию с правильным ответом. Если признаки слабо коррелируют, то они нам не нужны. Например колонка `Id` явно никак не влияет на стоимость дома.\n\nВам на вход передается изначальный **df_train** и **df** полученный из предыдущей задачи.\n\nВаша задача: \n\n* найти корреляцию всех **числовых** признаков **df_train** с признаком `SalePrice` с помощью `pd.corr`\n* если абсолютное значение корреляции признака с `SalePrice` меньше $0.05$ - удалите этот признак из **df**\n\nВерните измененный **df** и столбец корреляции признаков с признаком `SalePrice` упорядоченный по убыванию. Начало столбца корреляции выглядит следующим образом:\n\n|               |SalePrice |\n|---------------|----------|\n|**SalePrice**  |1.000000  |\n|**OverallQual**|0.790982  |\n|**GrLivArea**  |0.708624  |\n|**GarageCars** |0.640409  |\n|**GarageArea** |0.623431  |\n|**TotalBsmtSF**|0.613581  |\n|**1stFlrSF**   |0.605852  |\n|**FullBath**   |0.560664  |\n\nВсего должно получиться 37 числовых признаков."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def correlation(df: pd.DataFrame, df_train: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\none_hot_df = pd.read_csv('data/one_hot.csv')\ndf_train = pd.read_csv('data/train.csv')\n\ncorr_df, corr = correlation(one_hot_df, df_train)\n\nans_corr_df = pd.read_csv('data/corr_df.csv')\nans_corr = pd.read_csv('data/corr.csv').set_index('Unnamed: 0')\n\nassert_frame_equal(corr_df.astype('float64').reindex(sorted(corr_df.columns), axis=1),\n                    ans_corr_df.astype('float64').reindex(sorted(ans_corr_df.columns), axis=1))\n\nassert_array_almost_equal(corr.values, ans_corr.values, decimal=4)\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering и Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу 2 простые задачки:\n1.\nДавайте нагенерируем несколько фич во входном фрейме **df**:\n\n* `TotalArea` = `TotalBsmtSF` + `1stFlrSF` + `2ndFlrSF` + `GrLivArea` + `GarageArea`\n* `YearAverage` = (`YearRemodAdd` + `YearBuilt`) / 2\n* `LiveAreaQual` = `OverallQual` * `GrLivArea`\n\nНа выход отправьте **df** c тремя новымим столбиками. столбцы должны идти в том же порядки что указаны в списке в хвосте **df**.\n\n2.\n\nУ стандартного и нормального масштабирования есть одна проблема: она учитывает все признаки, даже те, которые изначально некорректны (шум, выбросы). Чтобы избавитьться от шумов и выбросов и корректно масштабировать выборку необходимо использовать [RobustScaling](https://scikit-learn.org/0.18/auto_examples/preprocessing/plot_robust_scaling.html).\n\nВаша задача - отмасштабировать полученный фрейм с помощью `RobustScaler`. И вернуть отмасштабированный массив (да, скалирование возвращает массив, а не DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n\ndef feature_en(df: pd.DataFrame) -> pd.DataFrame:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef scaling(df: pd.DataFrame) -> np.array:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\ncorr_df = pd.read_csv('data/corr_df.csv')\nfeature_df = feature_en(corr_df)\nans_feature_df = pd.read_csv('data/feature_df.csv')\n\nassert_frame_equal(feature_df.astype('float64').reindex(sorted(feature_df.columns), axis=1),\n                    ans_feature_df.astype('float64').reindex(sorted(ans_feature_df.columns), axis=1))\n######################################################\nfeature_df = pd.read_csv('data/feature_df.csv')\nscale_df = scaling(feature_df)\nans_scale_df = pd.read_csv('data/scale_df.csv').values\n\nassert_array_almost_equal(scale_df, ans_scale_df, decimal=6)\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Смешанная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, теперь мы готовы обучать модель! Осталось изучить последний интересный трюк - смешанные модели.\n\nВозьмем [2 регрессии](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b):\n\n* Ridge\n* Lasso\n \nНайдем оптимальные значения $\\alpha$ для обеих регрессий с помощью GridSearch.\n\nТеперь отправим обе модели с наилучшими параметрами в класс указанный снизу. Это класс смешения моделей. В нем параметр $\\beta \\in [0,1]$ - это коэффициент, с которым берется ответ одного классификаторв, а ответ второго - с коэффициентом $(1 - \\beta)$. Такая техника нередко позволяет добиться лучших результатов, чем одна модель.\n\nТеперь найдем наилучшее $\\beta$ для смешенной модели также с помощью GridSearch. Осталось получить **y_pred** с помощью наилучшей смешанной модели.\n\nНа вход вы получаете **X_scaled** из предыдущей задачи и **df_train** начальный. Мы подготовили за вас **X_train**, **y_train** и **X_test**. \n\nВ задаче необходимо минимизировать метрику `neg_mean_squared_log_error`. Для удобства мы возьмем `np.log1p(y_train)` и будем минимизировать метрику `neg_mean_squared_error`. Эту метрику необходимо минимизировать у всех 3-х GridSearch.\n\nНа выход отправьте GridSearch объект смешанной модели, а также результат **y_test**. (Не забудьте его проэкспоненциировать).\n\nМы выдаем вам ориентировачные параметры для каждого GridSearch. Вы можете увеличить перебор, чтобы получить лучшую модель.\n```python\nparams_ridge = {'alpha': np.arange(1, 20)}\nparams_lasso = {'alpha': np.logspace(-4, 3, num=8, base=10)}\nparams_blend = {'beta': np.linspace(0, 1, 11)}\n```\n\nПервые 2 GridSearch **не нужно** писать в функции: они могут работать достаточно долго и превысят лимит работы задачи на сервере. Найдите у себя локально наилучшие параметры и уже с этими параметрами создайте смешанную модель внутри функции. \n\nТакже не нужно сильно увеличивать перебор для $\\beta$ - того, что есть, более чем достаточно.\n\nP.S. Осталось сохранить файл с **y_test** и отправить его в [соревнование](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submit). \n\nУлучшайте свои результы пробуя другие модели и другие параметры. Уберите больше ненужных признаков, добавьте новые фичи. Экспериментируйте и дерзайте!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.base import BaseEstimator\nfrom sklearn.model_selection import GridSearchCV\n\nclass BlendRegressor(BaseEstimator): # предок класса классификаторов, чтобы можно было засунуть в GridSearch\n    def __init__(self, clf1, clf2, beta=0.5):\n        self.clf1 = clf1 \n        self.clf2 = clf2\n        self.beta = beta #параметр смешивания\n\n    def fit(self, X, y): #обучаем классификатор\n        self.X_ = X\n        self.y_ = y \n        self.clf1.fit(X, y)\n        self.clf2.fit(X, y)\n        return self\n\n    def predict(self, X): #возвращаем значения \n        return self.clf1.predict(X) * self.beta + self.clf2.predict(X) * (1 - self.beta)\n\n    \ndef learning(X_scaled: np.array, df_train: pd.DataFrame) -> (GridSearchCV, np.array):\n    X_train = X_scaled[0: len(df_train),]\n    X_test  = X_scaled[len(df_train): len(X_scaled)]\n    y_train = np.log1p(df_train['SalePrice'])\n    \n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    \n    return blend_gs, np.exp(y_test) - 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "######################################################\nfrom sklearn.metrics import mean_squared_log_error\nX_scaled = pd.read_csv('data/scale_df.csv').values\ndf_train = pd.read_csv('data/train.csv')\ny_test_my = pd.read_csv('data/submission.csv')['SalePrice']\n\ngs, y_test = learning(X_scaled, df_train)\n\nassert type(gs) is sklearn.model_selection.GridSearchCV\nassert gs.best_params_['beta'] > 0.3\nassert gs.best_params_['beta'] < 1\nassert type(gs.estimator.clf1) == sklearn.linear_model.Ridge or type(gs.estimator.clf1) == sklearn.linear_model.Lasso\nassert type(gs.estimator.clf2) == sklearn.linear_model.Ridge or type(gs.estimator.clf2) == sklearn.linear_model.Lasso\nassert (10 > gs.estimator.clf1.alpha > 2) or (0.2 > gs.estimator.clf1.alpha > 0.00005) \nassert (10 > gs.estimator.clf2.alpha > 2) or (0.2 > gs.estimator.clf2.alpha > 0.00005) \n\nassert mean_squared_log_error(y_test, y_test_my) < 0.01\n\n######################################################\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}