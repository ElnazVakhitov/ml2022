{"cells": [{"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nimport pandas as pd\nimport sklearn\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal, assert_almost_equal\nfrom sklearn.datasets import make_regression, make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom typing import List\n\n\nimport warnings\nwarnings.filterwarnings('ignore')"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Энтропия и критерий Джини"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$p_i$ - вероятность нахождения системы в _i_-ом состоянии. \n\n`Энтропия Шеннона` определяется для системы с _N_ возможными состояниями следующим образом \n$$ S = - \\sum_{i=1}^{N} p_i \\log_2 p_i $$\n\n`Критерий Джини (Gini Impurity)`. Максимизацию этого критерия можно интерпретировать как максимизацию числа пар объектов одного класса, оказавшихся в одном поддереве. \n\nВ общем случае критерий Джини считается как\n$$ G = 1 - \\sum_{k} (p_k)^2 $$\n\nНеобходимо посчитать, значения `Энтропии` и `критерия Джини`\n\nНа вход подается только таргет состоящий из _0_ и _1_ (бинарная классификация)\n\n### Sample 1\n#### Input:\n```python\ny = np.array([1,1,1,1,1,1,0,0,0,0])\n```\n#### Output\n```python\n0.971, 0.480\n```\nРезультат проверяется с точность до 3ех цифр после запятой."]}, {"cell_type": "code", "metadata": {}, "source": ["def gini_impurity(y: np.ndarray) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef entropy(y: np.ndarray) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef calc_criteria(y: np.ndarray) -> (float, float):\n    assert y.ndim == 1\n    return entropy(y), gini_impurity(y)\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\ny = np.array([1,1,1,1,1,1,0,0,0,0])\nassert_almost_equal(calc_criteria(y), (0.971, 0.480), decimal=3)\n######################################################\ny = np.array([1, 1, 1, 1, 0, 0, 0, 1, 1])\nassert_almost_equal(calc_criteria(y), (0.918, 0.444), decimal=3)\n######################################################\n\nassert_almost_equal(calc_criteria(np.zeros(10)), (0, 0))\nassert_almost_equal(calc_criteria(np.ones(10)), (0, 0))\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Information gain"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Вам надо реализовать функцию `inform_gain`, которая будет вычислять прирост информации для критерия (энтропия или критерий Джини) при разбиении выборки по признаку (threshold). \n\nПрирост информации при разбиении выборки по признаку _Q_ (например $ x \\leq 12$) определяется как.\n$$ IG(Q) = S_0 - \\sum_{i=1}^{q} \\frac{N_i}{N} S_i $$ \nгде $q$ - число групп после разбиения. $N_i$ - число элементов выборки, у которых признак _Q_ имеет _i_-ое значение.  \n\nИ написать функцию `get_best_threshold`, которая будет находить наилучшее разбиение выборки. Вам необходимо пройтись по всем уникальным значениям массива X. Можно использовать `np.unique(X)`.\n\nНе забудьте добавить свои функции из предыдущей задачи.\n\nНа вход подается:\n\n* $X$ - одномерный массив - значения признака;\n* $y$ - значения бинарных классов;\n* **criteria\\_func** - функция критерия, для которой вычисляется наилучшее разбиение;\n* **thr** - значение разбиения.\n\n### Sample 1 (Inform gain)\n\n#### Input:\n```python\nX = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\ny = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\nthreshold=3,\ncriteria_func=entropy\n```\n#### Output:\n```python\n0.61\n```\n\n### Sample 2 (Get best threshold)\n\n#### Input:\n```python\nX = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\ny = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\ncriteria_func=entropy\n```\n#### Output:\n```python\n4, 1\n```\nP.S.\n`theshold` разбивает выборку по правилу `X <= threshold`, `X > threshold`. Соответственно, если мы двигаемся по значениям X, то `threshold` в примере примет значение 4.\n\n**При нескольких наилучших вариантов разбиения мы выбираем вариант с максимальным значением threshold**.\n\n### Sample 3 (Get best threshold)\n\n#### Input:\n```python\nX = np.array([1,  1,  1,  1, -1, -1, -1, -1])\ny = np.array([0,  0,  1,  1,  0,  0,  1,  1])\ncriteria_func=entropy\n```\n#### Output:\n```python\nget_best_threshold(X, y, entropy)\n1, 0\n```\nВ данном примере при любом разбиении, score будет равен 0. Поэтому выбираем наибольшее значение threshold - 1."]}, {"cell_type": "code", "metadata": {}, "source": ["def entropy(y: np.ndarray) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef gini_impurity(y: np.ndarray) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef inform_gain(X: np.ndarray, y: np.ndarray, threshold: float, criteria_func=entropy) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef get_best_threshold(X: np.ndarray, y: np.ndarray, criteria_func=entropy) -> (float, float):\n    assert X.ndim == 1\n    assert y.ndim == 1\n    best_threshold = 0\n    best_score = 0\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ            \n    return best_threshold, best_score"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX = np.array([3, 9, 0, 4, 7, 2, 1, 6, 8, 5])\ny = np.array([0, 1, 0, 0, 1, 0, 0, 1, 1, 1])\n\nassert_almost_equal(entropy(y), 1)\n\nassert_almost_equal(inform_gain(X, y, 4.5, entropy), 1)\nassert_almost_equal(inform_gain(X, y, 3, entropy), 0.61, decimal=2)\n\nthr, score = get_best_threshold(X, y, entropy)\nassert thr == 4\nassert_almost_equal(score, 1)\n######################################################\nassert_almost_equal(inform_gain(X, y, 4.5, gini_impurity), 0.5)\nassert_almost_equal(inform_gain(X, y, 2, gini_impurity), 0.21, decimal=2)\n\nthr, score = get_best_threshold(X, y, gini_impurity)\nassert thr == 4\nassert_almost_equal(score, 0.5)\n######################################################\nX = np.array([1, -1, -2, -4, -7, 2, 12, 16, 18, 15,12,13,23, 4, 5, 10,  6, 12 ])\ny = np.array([0,  0,  0,  0,  0, 0,  1,  1,  1,  1, 1, 1, 1, 0, 0,  0,  1,  1])\n\nassert_almost_equal(entropy(y), 1)\n\nassert_almost_equal(inform_gain(X, y, 15, entropy), 0.19, decimal=2)\nassert_almost_equal(inform_gain(X, y, 3, entropy), 0.45, decimal=2)\n\nthr, score = get_best_threshold(X, y, entropy)\nassert thr == 10\nassert_almost_equal(score, 0.74, decimal=2)\n######################################################\nassert_almost_equal(inform_gain(X, y,  -1, gini_impurity), 0.14, decimal=2)\nassert_almost_equal(inform_gain(X, y,  12, gini_impurity), 0.19, decimal=2)\n\nthr, score = get_best_threshold(X, y, gini_impurity)\nassert thr == 10\nassert_almost_equal(score, 0.4)\n######################################################\nX = np.array([1,  1,  1,  1, -1, -1, -1, -1])\ny = np.array([0,  0,  1,  1,  0,  0,  1,  1])\n\nassert_almost_equal(entropy(y), 1)\n\nassert_almost_equal(inform_gain(X, y,  1, entropy), 0, decimal=2)\nassert_almost_equal(inform_gain(X, y, -1, entropy), 0, decimal=2)\n\nthr, score = get_best_threshold(X, y, entropy)\nassert thr == 1\nassert_almost_equal(score, 0, decimal=2)\n######################################################\n\nX = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny = np.array([1, 1, 1, 1, 0, 0, 0, 0, 1, 1,])\n\nthr, score = get_best_threshold(X, y, entropy)\n\nassert_almost_equal(inform_gain(X, y,  3, entropy), 0.41, decimal=2)\nassert_almost_equal(inform_gain(X, y,  7, entropy), 0.17, decimal=2)\n\nassert thr == 3\nassert_almost_equal(score, 0.41, decimal=2)\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Best Split "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Реализуйте функцию `find_best_split`, которая находит наилучшее разбиение по всем признакам. На вход подается обучающая выборка и функция критерий. Необходимо вернуть: индекс фичи, значение границы (threshold) и результат разбиение (information gain).\n\n**При нескольких наилучших вариантов фичей мы выбираем вариант с максимальным значением максимальным номером фичи, по порядку в X**.\n\nДобавьте сюда все свои функции из предыдущих задач.\n\n### Sample 1\n\n#### Input:\n```python\nX = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny = np.array([1, 1, 0, 0])\ncriteria_func=entropy\n```\n#### Output:\n```python\n0, -1.0, 1.0\n```\n\n### Sample 2\n\n#### Input:\n```python\nX = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny = np.array([1, 0, 1, 0])\ncriteria_func=entropy\n```\n#### Output:\n```python\n1, 1, 0\n```\n"]}, {"cell_type": "code", "metadata": {}, "source": ["def entropy(y: np.ndarray) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef gini_impurity(y: np.ndarray) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef inform_gain(X: np.ndarray, y: np.ndarray, threshold: float, criteria_func=entropy) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef get_best_threshold(X: np.ndarray, y: np.ndarray, criteria_func=entropy) -> (float, float):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ            \n    return best_threshold, best_score\n\ndef find_best_split(X, y, criteria_func=entropy):\n    assert X.ndim == 2\n    assert y.ndim == 1\n    best_feature = 0\n    best_score = 0\n    best_threshold = 0\n\n    ### делаем цикл по всем фичам в X и находим для каждой лучший threshold и score.\n    ### Потом сравниваем с наилучшим (best)\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    \n    return best_feature, best_threshold, best_score\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny_clf = np.array([1, 1, 0, 0])\n\nftr, thr, scr = find_best_split(X_clf, y_clf, entropy)\nassert ftr == 0\nassert thr == -1\nassert scr == 1\n######################################################\nX_clf = np.array([[1, 2, 3], [1, -1, 15], [1, 0, 2], [-1, 0, 4], [-1, 0, 21]])\ny_clf = np.array([1, 1, 0, 0, 1])\n\nftr, thr, scr = find_best_split(X_clf, y_clf, gini_impurity)\n\nassert ftr == 2\nassert thr == 4\nassert_almost_equal(scr, 0.21, decimal=2)\n######################################################\nX_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny_clf = np.array([1, 0, 1, 0])\n\nftr, thr, scr = find_best_split(X_clf, y_clf, entropy)\nassert ftr == 1\nassert thr == 1\nassert scr == 0\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Мое дерево решений"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ваша задача реализовать свой простой `DecisionTreeClassifier` для бинарных данных. Мы свели реализацию стандартных методов `fit`, `predict` и `predict_proba` к 2-м внутренним методам:\n\n* `_predict_proba_obj` - метод, принимающий 1 объект и возвращающий для него 2-мерный лист с вероятностями таргетов 0 и 1.\n* `_build_tree` - рекурсивный метод построения дерева. Каждая нода дерева представляет собой словарь. Формат словарей для вершин и листьев можно посмотреть в шаблоне кода.\n\nУ нашего классификатора будет лишь два гиперпараметра - максимальная глубина дерева `max_depth` и критерий разбиения `criterion`. Энтропия или Джини.\n\n**Все** функции из предыдущих заданий нужно добавить в этот код.\n\nНа вход будет подаваться выборка объектов $X$. $y$ - результат бинарной классификации $0$ или $1$.\n\n### Sample 1\n#### Input:\n```python\nX_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny_clf = np.array([1, 1, 0, 0])\n\nmodel = MyDecisionTreeClassifier(max_depth=2, criterion='entropy').fit(X_clf, y_clf)\ny_pred = model.predict(np.array([[2, 2], [-2, -2]]))\ny_prob = model.predict_proba(np.array([[2, 2], [-2, -2]]))\n```\n#### Output:\n```python\ny_pred = np.array([1, 0])\ny_prob = np.array([[0.0, 1.0], [1.0, 0.0]])\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["def entropy(y: np.ndarray) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\ndef gini_impurity(y: np.ndarray) -> float:\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass\n\nclass MyDecisionTreeClassifier():\n    def __init__(self, max_depth=4, criterion='entropy'): \n        self.max_depth = max_depth\n        self.criterion = criterion # 'entropy' or 'gini' \n        self.tree = {}\n        self._criteria_func = {\n            'gini': gini_impurity,\n            'entropy': entropy\n        }\n        self.count_of_features = 0\n\n    def _build_tree(self, X, y, depth=0):\n\n        split_feature, split_val, score = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・\n\n        # критерий остановки: если мы достигли нужной глубины или если скор оказался равен нулю\n        # технически со скором 0 нужно проверить потенциальную разбиваемость на след уровнях, но это сложно, так что пока так закостылим:)\n        if depth == self.max_depth or score == 0:\n            return {'leaf': True,\n                    'proba': ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・\n                   }\n\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・\n\n        left_tree  = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・\n        right_tree = ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・\n\n        return {'leaf': False,\n                'val': {'split_feature': split_feature,\n                        'split_val': split_val,\n                        'score': score},\n                'left': left_tree,\n                'right': right_tree}\n\n    def _predict_proba_obj(self, obj: np.array):\n        assert obj.ndim == 1\n\n        node = self.tree\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        ### необходимо спустится в нужную листовую ноду\n\n        assert node['leaf'] == True\n        return node['proba']\n        \n    def fit(self, X: np.ndarray, y: np.ndarray):\n        self.count_of_features = X.shape[1]\n        self.tree = self._build_tree(X, y, depth=0)\n        return self\n    \n    def predict_proba(self, X: np.ndarray):\n        assert X.shape[1] == self.count_of_features\n        proba = []\n        for row in X:\n            proba.append(self._predict_proba_obj(row))\n        return proba\n    \n    def predict(self, X: np.ndarray): # получаем \n        assert X.shape[1] == self.count_of_features\n        proba = np.array(self.predict_proba(X))\n        return list(np.argmax(proba, axis=1))\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX_clf = np.array([[1,1], [1, 1], [1, -1], [-1,-1], [-1,-1], [-1, 1]])\ny_clf = np.array([1, 1, 1, 0, 0, 0])\n\n\nmodel = MyDecisionTreeClassifier(max_depth=2).fit(X_clf, y_clf)\n\nassert_equal(model.predict(np.array([[2, 2], [-2, -2]])), np.array([1, 0]))\n\nassert_almost_equal(model.predict_proba(np.array([[2, 2], [-2, -2]])),\n                    np.array([[0.0, 1.0], [1.0, 0.0]]),decimal=2)\n######################################################\nX_clf = np.array([[-3, 0],\n                  [-3, 1],\n                  [-3, 2],\n                  [-3, 3],\n                  [-3, 4],\n                  [-3, 5],\n                  [-3, 6],\n                  [-3, 7],\n                  [-3, 8],\n                  [-3, 9],\n                  [3, 0],\n                  [3, 1],\n                  [3, 2],\n                  [3, 3],\n                  [3, 4],\n                  [3, 5],\n                  [3, 6],\n                  [3, 7],\n                  [3, 8],\n                  [3, 9],])\n\ny_clf = np.array([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0])\n\nmodel_d1 = MyDecisionTreeClassifier(max_depth=1).fit(X_clf, y_clf)\nmodel_d2 = MyDecisionTreeClassifier(max_depth=2).fit(X_clf, y_clf)\nmodel_d3 = MyDecisionTreeClassifier(max_depth=3).fit(X_clf, y_clf)\n\nassert_almost_equal(model_d1.predict_proba(np.array([[-3, 1.5], [-3, 6.5], [-3, 10],\n                                                     [3, 1.5], [3, 6.5], [3, 10]])),\n                    np.array([[0.4, 0.6], [0.4, 0.6], [0.4, 0.6],\n                              [0.6, 0.4], [0.6, 0.4], [0.6, 0.4]]),\n                    decimal=2)\n\nassert_almost_equal(model_d2.predict_proba(np.array([[-3, 1.5], [-3, 6.5], [-3, 10],[3, 1.5], [3, 6.5], [3, 10]])),\n                    np.array([[0.0, 1.0], [0.666, 0.333], [0.666, 0.333],\n                              [1.0, 0.0], [0.333, 0.666], [0.333, 0.666]]),\n                    decimal=2)\n\nassert_almost_equal(model_d3.predict_proba(np.array([[-3, 1.5], [-3, 6.5], [-3, 10],[3, 1.5], [3, 6.5], [3, 10]])),\n                    np.array([[0., 1.], [1., 0.], [0., 1.],\n                              [1., 0.], [0., 1.], [1., 0.]]),\n                    decimal=2)\n\n######################################################\n\nX, y = make_classification(n_samples=500, n_features=4, n_informative=2,\n                           scale=5, random_state=42)\nX = X.astype(np.int)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=42, stratify=y)\nmy_dtc = MyDecisionTreeClassifier(max_depth=4, criterion='entropy').fit(X_train, y_train)\nmy_acc = accuracy_score(y_test, my_dtc.predict(X_test))\n\nassert my_acc > 0.9\n######################################################\nmy_dtc = MyDecisionTreeClassifier(max_depth=4, criterion='gini').fit(X_train, y_train)\nmy_acc = accuracy_score(y_test, my_dtc.predict(X_test))\n\nassert my_acc > 0.9\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Bootstrap"]}, {"cell_type": "markdown", "metadata": {}, "source": ["На вход массив чисел $X$ и число бутстрепных выборок $B$. Необходимо реализовать свой бутстреп и найти матожидание и стандартную ошибку у бутстрепных выборок.\n\n### Sample 1\n#### Input:\n```python\nX = np.array([37,43,38,36,17,40,40,45,41,84])\nB = 100000\n```\n#### Output:\n```python\n42.1, 4.56\n```\n"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nfrom scipy.stats import sem # ищет SE среднего\n\ndef get_stats(X: np.array, B:int)->tuple:\n    '''\n        .∧＿∧ \n        ( ･ω･｡)つ━☆・*。 \n        ⊂  ノ    ・゜+. \n        しーＪ   °。+ *´¨) \n                .· ´¸.·*´¨) \n                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n    '''\n    return mean, SE"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX = np.array([37,43,38,36,17,40,40,45,41,84])\nB = 20000\n\nmean, se = get_stats(X, B)\n\nassert np.abs(mean - 42.1) < 0.1\nassert np.abs(se - 4.56) < 0.1\n######################################################\nX = np.array([76, 7, 72, 52, 56, 24, 27, 69, 26, 44, 11, 1, 62, 64, 40, 35, 42, 12, 84, 69, 79, 23, 17, 37, 3, 21, 8, 92, 87, 66, 22, 88, 3, 68, 98, 55, 11, 17, 16, 85, 71, 2, 32, 99, 84, 96, 57, 78, 57, 22, 81, 13, 87, 13, 95, 41, 38, 28, 47, 80, 84, 24, 7, 94, 81, 50, 55, 10, 73, 84, 68, 21, 96, 61, 24, 18, 23, 17, 32, 44, 60, 5, 96, 2, 53, 1, 37, 14, 81, 5, 74, 53, 29, 5, 70, 34, 29, 40, 81, 29])\n\nB = 20000\n\nmean, se = get_stats(X, B)\n\nassert np.abs(mean - 46.5) < 0.1\nassert np.abs(se - 3) < 0.1\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Bias-variance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["На вход подается **один** объект $(x, y)$ и список из нескольких **обученных** моделей. \n\nНеобходимо найти $error$, $bias^2$, $variance$ для данного объекта.\n\nТеперь все аккуратно запишем, чтобы не запутаться.\n\n* $(x, y)$ - тестировачная выборка\n* $a_1(\\cdot), \\ldots, a_M(\\cdot)$ - модели (это не обученные на бутстрепе модели, а просто возможные модели из пространства $\\mathbb{A}$, которое мы выбрали)\n\nКак настоящие статистики мы можем ~~забить~~ оценить матожидание как среднее. **Это не смешанная модель, а именно оценка матожидания через среднее**\n$$\\mathbb{E}a(x) = \\frac{1}{M}\\sum_{i=1}^{M}a_i(x)$$\n\n**Error** (берем матожидание от квадрата разности)\n\n$$error = \\mathbb{E}_{a}(a(x)-y)^2 = \\frac{1}{M}\\sum_{i=1}^{M}(a_i(x) - y)^2$$\n\n**Bias** (заметьте, что возвращаем квадрат bias, а не просто bias)\n\n$$bias^2 = \\Big(y - \\mathbb{E}_{a}[a(x)]\\Big)^2 = \\Big(y - \\frac{1}{M}\\sum_{i=1}^{M}a_i(x)\\Big)^2$$  \n\n\n**Variance** (ищем смещенную оценку)\n\n$$variance = \\mathbb{D}_{a}a(x)= \\mathbb{E}_{a}(a(x) - \\mathbb{E}_{a}a(x))^2 = \\frac{1}{M}\\sum_{i=1}^{M}\\Big(a_i(x)-\\frac{1}{M}\\sum_{r=1}^{M}a_r(x)\\Big)^2$$\n\n### Sample 1\n#### Input:\n```python\nX, y = np.array([[0,0,0]]), 0\nestimators = [DecisionTreeRegressor(max_depth=3, random_state=1),  #already fitted estimators\n              DecisionTreeRegressor(max_depth=5, random_state=1)]\n```\n#### Output:\n```python\nerror, bias2, var = 3.574, 3.255, 0.319\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n\ndef bias_variance_decomp(X_test:np.array, y_test:int, estimators:list)->tuple:\n    '''\n        .∧＿∧ \n        ( ･ω･｡)つ━☆・*。 \n        ⊂  ノ    ・゜+. \n        しーＪ   °。+ *´¨) \n                .· ´¸.·*´¨) \n                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n    '''\n\n    return error, bias2, var"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.tree import DecisionTreeRegressor\n\ndef generate(n_samples, noise, f):\n    X = np.linspace(-4, 4, n_samples)\n    y = f(X)\n    X = X.reshape((n_samples, 1))\n\n    return X, y\n######################################################\n\nn_train = 150        \nnoise = 0.1\n\n# Generate data\ndef f(x):\n    x = x.ravel()\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n\nX_fit, y_fit = generate(n_samples=n_train, noise=noise, f=f)\n\nestimators = [DecisionTreeRegressor(max_depth=2, random_state=1).fit(X_fit, y_fit),\n              DecisionTreeRegressor(max_depth=4, random_state=1).fit(X_fit, y_fit)]\n\nX_test, y_test = np.array([[2]]), 1.5\n\nerror, bias, var = bias_variance_decomp(X_test, y_test, estimators)\n\nassert_array_almost_equal(np.array([error, bias, var]), \n                          np.array([0.108, 0.083, 0.025]), decimal=3)\n\nX_test, y_test = np.array([[-0.7]]), 0.8\nerror, bias, var = bias_variance_decomp(X_test, y_test, estimators)\n\nassert_array_almost_equal(np.array([error, bias, var]), \n                          np.array([0.045, 0.002, 0.043]), decimal=3)\n\n######################################################\n\nX_fit, y_fit = make_regression(n_samples=1000, n_features=3, n_informative=3, bias=2, noise=10,\n                       n_targets=1, shuffle=False, random_state=10)\n\nestimators = [DecisionTreeRegressor(max_depth=3, random_state=1).fit(X_fit, y_fit),\n              DecisionTreeRegressor(max_depth=5, random_state=1).fit(X_fit, y_fit)]\n\nX_test, y_test = np.array([[0,0,0]]), 0\nerror, bias, var = bias_variance_decomp(X_test, y_test, estimators)\n\nassert_array_almost_equal(np.array([error, bias, var]), \n                          np.array([3.574, 3.255, 0.319]), decimal=3)\n\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Bias-variance v2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["А теперь тоже самое, только для нескольких объектов\n\nНа вход подается тестовая выборка объект $(X_{test}, y_{test})$ и список из нескольких **обученных** моделей. \n\nНеобходимо найти $error$, $bias^2$, $variance$ для данного объекта.\n\n$$error = \\mathbb{E}_{x,y}\\mathbb{E}_{a}(a(x)-y)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{M}\\sum_{j=1}^{M}(a_j(x_i) - y_i)^2$$\n\n$$bias^2 = \\mathbb{E}_{x,y}\\Big(y - \\mathbb{E}_{a}[a(x)]\\Big)^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\Big(y_i - \\frac{1}{M}\\sum_{j=1}^{M}a_j(x_i)\\Big)^2$$  \n\n$$variance = \\mathbb{E}_{x,y}\\mathbb{D}_{a}a(x)= \\mathbb{E}_{x,y}\\mathbb{E}_{a}(a(x) - \\mathbb{E}_{a}a(x))^2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{1}{M}\\sum_{j=1}^{M}\\Big(a_j(x_i)-\\frac{1}{M}\\sum_{r=1}^{M}a_r(x_i)\\Big)^2$$\n\n\n### Sample 1\n#### Input:\n```python\nx = np.array([[  0,   0,   0],\n              [0.1, 0.1, 0.1]])\ny = np.array([0, 0.1])\n\nestimators = [DecisionTreeRegressor(max_depth=3, random_state=3), \n              DecisionTreeRegressor(max_depth=5, random_state=3)]\n```\n#### Output:\n```python\nerror, bias2, var = 3.399, 3.079, 0.319\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n\ndef bias_variance_decomp2(x_test:np.array, y_test:np.array, estimators:list)->tuple:\n    '''\n        .∧＿∧ \n        ( ･ω･｡)つ━☆・*。 \n        ⊂  ノ    ・゜+. \n        しーＪ   °。+ *´¨) \n                .· ´¸.·*´¨) \n                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n    '''\n\n    return error, bias2, var"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.tree import DecisionTreeRegressor\n\ndef generate(n_samples, noise, f):\n    X = np.linspace(-4, 4, n_samples)\n    y = f(X)\n    X = X.reshape((n_samples, 1))\n\n    return X, y\n######################################################\n\n# Generate data\ndef f(x):\n    x = x.ravel()\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n\nX_fit, y_fit = generate(n_samples=150, noise=0.1, f=f)\n\nestimators = [DecisionTreeRegressor(max_depth=2, random_state=1).fit(X_fit, y_fit),\n              DecisionTreeRegressor(max_depth=4, random_state=1).fit(X_fit, y_fit)]\n\nX_test = np.array([[2], [-0.7]])\ny_test = np.array([1.5, 0.8])\n\nerror, bias, var = bias_variance_decomp2(X_test, y_test, estimators)\n\nassert_array_almost_equal(np.array([error, bias, var]), \n                          (np.array([0.108, 0.083, 0.025]) + np.array([0.045, 0.002, 0.043])) / 2, decimal=3)\n\n######################################################\n\nX_fit, y_fit = make_regression(n_samples=1000, n_features=3, n_informative=3, bias=2, noise=10,\n                       n_targets=1, shuffle=False, random_state=10)\n\nestimators = [DecisionTreeRegressor(max_depth=3, random_state=1).fit(X_fit, y_fit),\n              DecisionTreeRegressor(max_depth=5, random_state=1).fit(X_fit, y_fit)]\n\nX_test = np.array([[  0,   0,   0],\n              [0.1, 0.1, 0.1]])\ny_test = np.array([0, 0.1])\n\nerror, bias, var = bias_variance_decomp2(X_test, y_test, estimators)\n\nassert_array_almost_equal(np.array([error, bias, var]), \n                          np.array([3.399, 3.079, 0.319]), decimal=3)\n\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Bagging"]}, {"cell_type": "markdown", "metadata": {}, "source": ["На вход подается некий **необученный** алгоритм регрессии, тренировочная и тестовые выборки и число бутстрепных выборок.\n\nНеобходимо \n\n* бустингом сделать несколько выборок $X_1, \\ldots, X_B$\n* обучить алгоритмы на этих выборках: $a_1(\\cdot), \\ldots, a_B(\\cdot)$\n* реализовать бэггинг алгоритмов и найти собственно предсказания, $error$, $bias^2$ и $variance$.\n\nВот теперь аккуратно. Это - **не матожидание**! Это модель такая.\n$$a(x) = \\frac{1}{B}\\sum_{b=1}^{B}a_b(x)$$\n\nА вот ее матожидание равно для всех алгоритмов:\n$$\\mathbb{E}_aa(x) = \\mathbb{E}_a\\frac{1}{B}\\sum_{b=1}^{B}a_b(x) = \\mathbb{E}_aa_1(x)$$\n\nНо так как теперь, нам нужно посчитать матожидание, мы воспользуемся нашим множеством алгоритмов, обученных на бутстрепе, чтобы получить оценку матожидания единичного алгоритма.\n\n$$\\mathbb{E}_aa_1(x) = \\frac{1}{B}\\sum_{j=1}^{B}a_j(x)$$\n\nОстальные формулы берутся из предыдущей задачи.\n\nP.S.\n\n* Так как тут есть вероятности, в целом тесты могут `редко` не взлететь. Перезашлите задачу в этом случае.\n\n### Sample 1\n#### Input:\n```python\nboot_count = 1000\nestimators = [DecisionTreeRegressor(max_depth=2) for _ in range(boot_count)]\nX_train = np.array([[0, 0], [1, 1], [5, 5], [8, 8], [10, 10]])\ny_train = np.array([0, 1, 5, 8, 10])\nX_test  = np.array([[4, 4], [6, 6]])\ny_test  = np.array([4, 6])\n```\n#### Output:\n```python\ny_pred = np.array([3.656 6.039])\nerror  = 3.7 \nbias^2 = 0.1\nvar    = 3.7\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["def _draw_bootstrap_sample(X: np.array, y: np.array):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・\n\n    return X_boot, y_boot\n\ndef bagging(estimators, X_train: np.array, y_train: np.array, X_test: np.array, y_test: np.array):\n\n    for i, estimator in enumerate(estimators):\n        X_boot, y_boot = _draw_bootstrap_sample(X_train, y_train)\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・\n        # собрать большой 2-мерный массив предсказаний каждого классификатора\n\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・\n    # а вот сюда вставить код из предыдущей задачи\n\n    return y_pred, loss, bias2, var"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n\n######################################################\nboot_count=1000\nestimators = [DecisionTreeRegressor(max_depth=2) for _ in range(boot_count)]\nX_train = np.array([[0, 0], [1, 1], [5, 5], [8, 8], [10, 10]])\ny_train = np.array([0, 1, 5, 8, 10])\nX_test  = np.array([[4, 4], [6, 6]])\ny_test  = np.array([4, 6])\n\n\ny_pred, loss, bias, var = bagging(estimators, X_train, y_train, X_test, y_test)\n\n# Да я в курсе что очень грубые ограничения, просто пример игрушечный на таком малом количестве данных\nassert_array_almost_equal(y_pred, np.array([4, 6]), decimal=0)\n\nassert_almost_equal(loss, 3.7, decimal=0) \nassert_almost_equal(bias, 0.1, decimal=1) \nassert_almost_equal(var,  3.7, decimal=0) \n\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# RF Classification"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Осталось переделать чуток предыдущую задачу в `RandomForest`. \nНо теперь мы наконец попробуем классификацию. (Пока только бинарную)\n\nПлан\n\n* Также делаем бутстрепные выборки (для каждого дерева своя выборка, соответственно кол-во выборок - `n_estimators`, в каждой элементов как в начальной выборке `X_train`)\n* Бэггинг теперь будет только по деревьям классификации\n* Будем передавать параметр `n_estimators`, `max_depth` и `max_features`\n\nКак выбирать ответ в задаче классификации?\n\n* Для каждого внутреннего дерева решений находим веротности обоих классов для каждого объекта $X_{test}$:\n  * Вызываем `predict_proba` у `DecisionTreeClassifier`\n* Усредняем вероятности класса и объекта по деревьям:\n  * $P(n_{class}=d, object=x_k) = \\frac{1}{B}\\sum_{i=1}^{B}P(n_{class}=d, object=x_k, tree=b_i)$\n* Для каждого объекта выбираем тот класс, у которого выше вероятность\n\n\n\n### Sample 1\n#### Input:\n```python\nX_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\ny_train = np.array([0, 0, 1, 1])\nX_test  = np.array([[3, 3], [6, 6]])\ny_test  = np.array([0, 1])\n\nB = 1000\n```\n#### Output:\n```python\nmodel.predict(X_test) == np.array([0, 1])\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["class MyRFC():\n    def __init__(self, n_estimators=10, max_features=None, max_depth=None):\n        self.estimators_ = [DecisionTreeClassifier(max_depth=max_depth,\n                                                   max_features=max_features,\n                                                   random_state=np.random.randint(10000))\n                            for _ in range(n_estimators)]\n\n    def fit(self, X_train: np.array, y_train: np.array):\n        for estimator in self.estimators_:\n            ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n            ### сгенерировать бутстрепные выборки и обучить каждый классификатор на своей выборке\n        return self\n    \n    def predict_proba(self, X_test)-> np.array:\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        ### вернуть результат каждого классификатора и усреднить веротяности по формуле из условия\n        pass\n\n    def predict(self, X_test) -> np.array:\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        ### по стандарту вернуть argmax от predict_proba\n        pass"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX_train = np.array([[0, 0], [4, 4], [5, 5], [10, 10]])\ny_train = np.array([0, 0, 1, 1])\nX_test  = np.array([[3, 3], [6, 6]])\ny_test  = np.array([0, 1])\n\nB = 1000\n\ny_pred_my = MyRFC(n_estimators = 100, max_depth=3).fit(X_train, y_train).predict(X_test)\n\nassert_array_almost_equal(y_pred_my, np.array([0, 1]))\n######################################################\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nnum_samples = 1000\ntheta = np.linspace(0, 2*np.pi, num_samples)\n\nr1 = 1\nr2 = 2\n\nrng = np.random.RandomState(1)\n\ncircle = np.hstack([np.cos(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8), \n                    np.sin(theta).reshape((-1, 1)) + (rng.randn(num_samples)[:,np.newaxis] / 8)])\nlil = r1 * circle\nbig = r2 * circle\nX = np.vstack([lil, big])\ny = np.hstack([np.zeros(num_samples), np.ones(num_samples)])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.3,\n                                                    random_state=123,\n                                                    shuffle=True)\n\ny_test = y_test.astype('int')\n\ny_pred_my = MyRFC(n_estimators = 100, \n                  max_depth=1).fit(X_train, y_train).predict(X_test)\n\nassert accuracy_score(y_pred_my, y_test) > 0.85\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Feature Importance"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Просто верните отсортированный массив важности фич, полученные из обученного RandomForest. Фичи нумеруются с 1.\n\n### Sample 1\n#### Input:\n```python\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = np.array([0, 0, 1, 1])\n```\n#### Output:\n```python\nfeatures= np.array([1, 2])\nimportance = np.array([0.75, 0.25])\n\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["def feature_importance(X, y):\n    '''\n        .∧＿∧ \n        ( ･ω･｡)つ━☆・*。 \n        ⊂  ノ    ・゜+. \n        しーＪ   °。+ *´¨) \n                .· ´¸.·*´¨) \n                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n    '''\n    return features, importance"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX = np.array([[0, 0], [0,1], [1, 0], [1, 1]])\ny = np.array([0, 0, 1, 1])\n\nf, i = feature_importance(X, y)\n\nassert_array_equal(f , np.array([1, 2]))\nassert i[0] > 0.74\n######################################################\n"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.9"}}, "nbformat": 4, "nbformat_minor": 4}