{"cells": [{"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nimport sklearn\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal\nfrom typing import Tuple\n\nimport warnings\nwarnings.filterwarnings('ignore')"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Матричные производные"]}, {"cell_type": "markdown", "metadata": {}, "source": ["На вход подаются вектор $\\boldsymbol{x}$ длины $n$, $\\boldsymbol{c}$ длины $m$ и матрица $A$ размера $\\{n\\times m\\}$\n\nПусть $y = \\boldsymbol{x}^TA\\boldsymbol{c}$\n\nНайдите $\\frac{dy}{d\\boldsymbol{x}}$, $\\frac{dy}{dA}$ и $\\frac{dy}{d\\boldsymbol{c}}$"]}, {"cell_type": "code", "metadata": {}, "source": ["def matrix_deriv(x: np.array, A: np.array, c: np.array):\n    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n    pass"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nx = np.array([ [1], [2], [3],  [4]])\nc = np.array([[25], [5],[-9]])\nA = np.arange(12).reshape(4,3)\n\ny1, y2, y3 = matrix_deriv(x, A, c)\n\nassert_array_equal(y1, np.array([[-13],\n                                [ 50],\n                                [113],\n                                [176]]))\n\nassert_array_equal(y2, np.array([[ 25,   5,  -9],\n                                [ 50,  10, -18],\n                                [ 75,  15, -27],\n                                [100,  20, -36]]))\n\nassert_array_equal(y3, np.array([[60, 70, 80]]))\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Честная регрессия"]}, {"cell_type": "markdown", "metadata": {}, "source": ["В случае линейной регрессии по функционалу $MSE$ задачу оптимизации можно записать следующим образом:\n\n$$ \\frac{1}{n}\\sum_{k=1}^N (\\boldsymbol{x}_k^T\\boldsymbol{w} - y_k) ^ 2 \\to \\min_{\\boldsymbol{w}}$$\n\nМы уже знаем, что решение будет выглядеть следующим образом:\n\n$$\\boldsymbol{w} = \\begin{cases} X^{-1}\\boldsymbol{y}, & n = m\\\\\n(X^TX)^{-1}X^T\\boldsymbol{y}, & n > m\\\\\nX^{T}(XX^{T})^{-1}\\boldsymbol{y}, & n < m\n\\end{cases}$$\n\nгде $n$ - количество объектов, а $m$ - количество признаков + 1 (дополнительно 1 вес без признака).\n\nРешите задачу регрессии честно, используя формулу выше. \n\nЧтобы вы не запутались мы сразу написали добавление единичного столбца к матрице $X$.\n\n\n### Sample 1\n#### Input:\n```python\nX_train = np.array([[1]])\ny_train = np.array([2])\nX_test = np.array([[0.], [2.], [3.]])\n```\n#### Output:\n```python\nmodel.predict(X_test) == np.array([1., 3., 4.])\nmodel.w_ == np.array([[1.], [1.]])\nmodel.coef_ == np.array([1., 1.])\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["class TrueLinReg():\n    def __init__(self):\n        self.w_ = None  # столбец\n        self.coef_ = None # строка\n\n    def fit(self, X: np.array, y: np.array) -> np.array:\n        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1) # добавляем вес без признака\n\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n\n        self.coef_ = self.w_.reshape(-1)\n        return self\n\n    def predict(self, X) -> np.array:\n        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n        return (X @ self.w_).reshape(-1)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\n######################################################\nX_reg = np.array([[1], [2]])\ny_reg = np.array([1, 2])\n\nmodel = TrueLinReg().fit(X_reg, y_reg)\n\nassert_array_almost_equal(model.predict(np.array([[0], [3], [4]])), np.array([0, 3, 4]), decimal=2)\n\nassert_array_almost_equal(model.w_, np.array([[1.], [0.]]), decimal=2)\n\nassert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=2)\n######################################################\nX_reg = np.array([[1], [2], [3]])\ny_reg = np.array([1, -2, 1])\n\nmodel = TrueLinReg().fit(X_reg, y_reg)\nassert_array_almost_equal(model.predict(np.array([[0],[4]])), np.array([0., 0.]), decimal=2)\n\nassert_array_almost_equal(model.w_, np.array([[0.], [0.]]), decimal=2)\n\nassert_array_almost_equal(model.coef_, np.array([0., 0.]), decimal=2)\n######################################################\nX_reg = np.array([[1]])\ny_reg = np.array([2])\n\nmodel = TrueLinReg().fit(X_reg, y_reg)\nassert_array_almost_equal(model.predict(np.array([[0.], [2.], [3.]])), np.array([1., 3., 4.]), decimal=2)\n\nassert_array_almost_equal(model.w_, np.array([[1.], [1.]]), decimal=2)\n\nassert_array_almost_equal(model.coef_, np.array([1., 1.]), decimal=2)\n######################################################\nX_reg, y_reg = make_regression(n_samples=10, n_features=15, n_targets=1)\n\nmodel = LinearRegression().fit(X_reg, y_reg)\nmodel_my = TrueLinReg().fit(X_reg, y_reg)\ncoef_real = np.hstack([model.coef_, model.intercept_])\n\nassert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 20\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Честная регуляризация"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Добавьте регуляризацию с коэффициентом $\\lambda$ и решите задачу регрессии (для любого соотношения $m$ и $n$ используйте формулу)\n$$\\boldsymbol{w} = (X^TX + \\lambda E)^{-1}X^T\\boldsymbol{y}$$\nгде $E$ - единичная матрица.\n\nНе забудьте сами добавить **справа** единичный столбец к матрице $X$ аналогично предыдущей задаче.\n### Sample 1\n#### Input:\n```python\nX_train = np.array([[1], [2]])\ny_train = np.array([1, 2])\nlamb = 1\n\nX_test = np.array([[0], [4]])\n```\n#### Output:\n```python\nmodel.predict(X_test) == np.array([0.33, 3])\nmodel.w_ == np.array([[0.67], [0.33]])\nmodel.coef_ == np.array([0.67, 0.33])\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["class TrueReg():\n    def __init__(self, lamb):\n        self.lamb = lamb\n        \n    def fit(self, X, y):\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass\n        \n    def predict(self, X):\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression, Ridge\n######################################################\nX_reg = np.array([[1], [2]])\ny_reg = np.array([1, 2])\n\nmodel = TrueReg(1).fit(X_reg, y_reg)\n\nassert_array_almost_equal(model.predict(np.array([[0], [4]])), np.array([0.33, 3]), decimal=2)\n\nassert_array_almost_equal(model.w_, np.array([[0.67], [0.33]]), decimal=2)\n\nassert_array_almost_equal(model.coef_, np.array([0.67, 0.33]), decimal=2)\n######################################################\nX_reg, y_reg = make_regression(n_samples=100, n_features=99, n_targets=1)\n\nmodel = Ridge(1).fit(X_reg, y_reg)\nmodel_my = TrueReg(1).fit(X_reg, y_reg)\ncoef_real = np.hstack([model.coef_, model.intercept_])\n\nassert_array_almost_equal(model_my.coef_, coef_real, decimal=0)\nassert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 1\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Градиент"]}, {"cell_type": "markdown", "metadata": {}, "source": ["На вход подаются обучающая выборка $(X,\\boldsymbol{y})$ и как-то определенные веса $w$. Верните градиент функции $MSE$ для изменения весов в алгоритме градиентного спуска.\n$$\\nabla_{\\boldsymbol{w}} L = \\frac{2}{n} X^{T}(X\\boldsymbol{w} - \\boldsymbol{y})$$\n\nЗдесь не нужно добавлять единицы сбоку к матрице, считаем, что они уже есть.\n\n### Sample 1\n#### Input:\n```python\nw = np.array([[0.2], [0.2]]) # столбец!\nX = np.array([[1,1], [2,2], [3,3]])\ny = np.array([[1],[2],[3]]) # столбец!\n```\n#### Output:\n```python\narray([[-5.6],\n       [-5.6]]) # возвращаем столбец!\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["def gradient_step(w: np.array, X: np.array, y: np.array) -> np.array:\n    '''\n        .∧＿∧ \n        ( ･ω･｡)つ━☆・*。 \n        ⊂  ノ    ・゜+. \n        しーＪ   °。+ *´¨) \n                .· ´¸.·*´¨) \n                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n    '''\n    #нужно вернуть столбец\n    pass"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nw = np.array([[0.2], \n              [0.2]])\nX = np.array([[1,1], [2,2], [3,3]])\ny = np.array([[1],[2],[3]])\nassert_array_almost_equal(gradient_step(w, X, y), np.array([[-5.6], [-5.6]]), decimal=3)\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Стохастический градиент"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Реализуйте стохаистический градиентный спуск: пересчитайте $\\nabla_{\\boldsymbol{w}}L$ только для одного случайно выбранного элемента из выборки $X$.\n\n$$L_{k} = (\\boldsymbol{x}_k^T \\boldsymbol{w} - y_k) ^ 2 \\to \\min_{\\boldsymbol{w}}$$\n\n$$\\nabla_{\\boldsymbol{w}}L_{k} = \\boldsymbol{x}_k (\\boldsymbol{x}_k^T \\boldsymbol{w}  - y_k)$$\nгде $\\boldsymbol{x}_i$ - вектор объекта выборки, выбранный случайно.  \n\n\nЗдесь не нужно добавлять единицы сбоку к матрице $X$, считаем, что они уже есть.\n\n### Sample 1\n#### Input:\n```python\nw = np.array([[1], [1], [1]]) #столбец!\nX = np.array([[1, 1, 1], [0, 0, 0]])\ny = np.array([[1], [0]]) #столбец!\n```\n#### Output:\n```python\narray([[0.],\n       [0.],\n       [0.]]) #столбец!\nили\narray([[2.],\n       [2.],\n       [2.]]) #столбец!\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n\ndef stochastic_step(w: np.array, X: np.array, y: np.array) -> np.array:\n    '''\n        .∧＿∧ \n        ( ･ω･｡)つ━☆・*。 \n        ⊂  ノ    ・゜+. \n        しーＪ   °。+ *´¨) \n                .· ´¸.·*´¨) \n                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n    '''\n    #нужно вернуть столбец\n    pass"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nw = np.array([[1], [1], [1]])\ny = np.array([[1], [0]])\nX = np.array([[1, 1, 1], [0, 0, 0]])\n\nz , k = 0 , 0\nfor i in range(100):\n    g = stochastic_step(w, X, y)\n    if g[0] == 2:\n        z += 1\n    else:\n        k += 1\nassert z >= 35 and k >= 35\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Градиентный спуск"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Теперь реализуем Линейную регрессию на градиентном спуске. Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса инициализированы нулями. \n$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta\\nabla_{\\boldsymbol{w}} L$$\n\nИспользуйте параметры `max_iter`=$1000$, `eta`=$0.1$\n\nВсе итерации проходить не нужно. Остановитесь в момент, когда норма разницы между старыми и новыми весами станет меньше $1e-9$. \n\n\n### Sample 1\n#### Input:\n```python\nX_train = np.array([[1], [2]])\ny_train = np.array([1, 2])\n\nmodel = GDLinReg(max_iter=1000, eta=0.1).fit(X_train, y_train)\ny_pred = model.predict(np.array([[3],[4]]))\n\n```\n#### Output:\n```python\ny_pred = np.array([3., 4.])\nmodel.coef_ = np.array([1., 0.])\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["class GDLinReg():\n    def __init__(self, max_iter=1000, eta=0.1):\n        self.max_iter = max_iter\n        self.eta = eta\n        self.coef_ = None #строка\n        self.w_ = None #столбец\n    \n    def _gradient_descending(w, X, y):\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass\n    \n    def fit(self, X, y):\n        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n        n, m = X.shape\n        self.w_ = np.zeros((m, 1)) #столбец\n        \n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        \n        self.coef_ = self.w_.reshape(-1)\n        return self\n        \n    def predict(self, X):\n        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)   \n        return (X @ self.w_).reshape(-1) #возвращаем всегда строку"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.datasets import make_regression\n######################################################\nX_reg = np.array([[1], [2]])\ny_reg = np.array([1, 2])\n\nmodel = GDLinReg().fit(X_reg, y_reg)\nassert_array_almost_equal(model.predict(np.array([[3],[4]])), np.array([3., 4.]), decimal=1)\n\nassert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=1)\n######################################################\nX_reg, y_reg = make_regression(n_samples=200, n_features=10, n_targets=1)\n\nmodel = LinearRegression().fit(X_reg, y_reg)\nmodel2 = GDLinReg().fit(X_reg, y_reg)\n\ncoef_real = np.hstack([model.coef_, model.intercept_])\ncoef_my = model2.coef_\n\nassert mean_absolute_error(coef_my, coef_real) < 5\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Регуляризация"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Реализуйте L2-регуляризацию (так же известную как Ridge).\n\n$$L = \\frac{1}{2}\\lVert X\\boldsymbol{w} - \\boldsymbol{y}\\rVert_2^2 + \\frac{\\lambda}{2}\\lVert \\boldsymbol{w} \\rVert_2^2$$\n\nНайдите новый $\\nabla_{\\boldsymbol{w}} L$ и верните его.\n\nОбратите внимание, что свободный коэффициент весов (самый последний) **не нужно** регуляризовывать.\n\n### Sample 1\n#### Input:\n```python\nw = np.array([[1], [1]])\nX = np.array([[1,1], [2,2], [3,3]])\n\ny = np.array([[1],[0],[0]])\n```\n#### Output:\n```python\nnp.array([[28.], \n          [27.]]) #возвращаем столбец!\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["def gradient_step_l2(w: np.array, X: np.array, y: np.array, lamb: float) -> np.array:\n    # Коэффициент регуляризации lamb\n    '''\n        .∧＿∧ \n        ( ･ω･｡)つ━☆・*。 \n        ⊂  ノ    ・゜+. \n        しーＪ   °。+ *´¨) \n                .· ´¸.·*´¨) \n                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n    '''\n    #нужно вернуть столбец\n    pass"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nw = np.array([[1], [1]])\nX = np.array([[1,1], [2,2], [3,3]])\n\ny = np.array([[1],[0],[0]])\nassert_array_almost_equal(gradient_step_l2(w, X, y, 1), np.array([[28.], [27.]]))\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Логистический градиент"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Теперь переходим к задаче классификации и Логистической регрессии. Мы уже знаем, что в нашем случае нужно минимизировать метрику $LogLoss$:\n$$L = -\\ln{Likelihood} = -\\sum_{k=1}^{n}\\Big(y_k\\ln{(\\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + (1 - y_k)\\ln{(1 - \\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))\\Big)}$$\nгде $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$\n\nВаша задача взять **производную** этой функции и вернуть вектор градиентов $\\nabla_{\\boldsymbol{w}}\\ln{L}$. Формулу необходимо вывести в векторном виде, чтобы считалось быстрее.\n\nОбратите внимание: \n\n* $w$, $y$ - столбцы, а не строки. Ответ тоже столбец.\n* В функции используется **натуральный** логарифм.\n* Нужно посчитать полный градиент, а не стохастический.\n\n### Sample 1\n#### Input:\n```python\nw = np.array([[0], [0]])\nX = np.array([[1,1], [2,2], [3,3]])\ny = np.array([[1],[0],[0]])\n```\n#### Output:\n```python\narray([[2], \n       [2]]) #возвращаем столбец!\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n\ndef log_gradient_step(w: np.array, X: np.array, y: np.array)-> np.array:\n    '''\n        .∧＿∧ \n        ( ･ω･｡)つ━☆・*。 \n        ⊂  ノ    ・゜+. \n        しーＪ   °。+ *´¨) \n                .· ´¸.·*´¨) \n                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n    '''\n    #нужно вернуть столбец\n    pass"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\nfrom numpy.testing import assert_array_almost_equal\n\n######################################################\nw = np.array([[0], [0]])\nX = np.array([[1,1], [2,2], [3,3]])\n\ny = np.array([[1],[0],[0]])\nassert_array_almost_equal(log_gradient_step(w, X, y), np.array([[2], [2]]))\n######################################################\nw = np.array([[1], [2]])\nX = np.array([[11,21], [12,22], [13,31]])\n\ny = np.array([[3],[2],[10]])\nassert_array_almost_equal(log_gradient_step(w, X, y), np.array([[-151.],\n                                                                [-343.]]))\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Логистическая регрессия"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Теперь реализуем Логистическую регрессию на стохастическом градиентном спуске.\n$$L = -\\ln{Likelihood} = -\\sum_{k=1}^{n}\\Big(y_k\\ln{(\\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + (1 - y_k)\\ln{(1 - \\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))\\Big)} + \\frac{\\lambda}{2}\\lVert \\boldsymbol{w} \\rVert_2^2 \\rightarrow \\min$$\nгде $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$, $\\lVert \\boldsymbol{w} \\rVert_2^2 = \\sum_{i=1}^{m} w_i^2$= - квадрат эвклидовой нормы\n\nСобственно вероятность принадлежности определенному классу:\n$$P(y_k=1) = \\sigma(\\boldsymbol{x}_k^T \\boldsymbol{w})~~~P(y_k=0) = 1 - \\sigma(\\boldsymbol{x}_k^T \\boldsymbol{w})$$\n\nРеализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса сгенерированны рандомно. \n\n$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta\\nabla_{\\boldsymbol{w}} L$$\n\nБудьте внимательны:\n\n* На вход алгоритму приходит $\\boldsymbol{y}$ - **строка**, как и в любой sklearn алгоритм.\n* Не устанавливайте количество итераций больше 1000, так как алгоритм будет долго работать.\n* Как и в линейной регрессии не забудьте добавить единичный столбец справа.\n\nМожете использовать и обычный спуск, главное чтоб по времени зашло.\n\n### Sample 1\n#### Input:\n```python\nX_train = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny_train = np.array([1, 1, 0, 0])\n\nmodel = SGDLogReg().fit(X_train, y_train)\ny_pred = model.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]]))\ny_prob = model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n```\n#### Output:\n```python\ny_pred = np.array([1., 0.])\ny_prob = np.array([[0.1, 0.9],  # это не точный ответ, но очень приблизительный, отличие на 0.1 - это нормально\n                   [0.9, 0.1]]) # для каждого объекта возвращаем его вероятность нуля и единицы\n```"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n\nclass SGDLogReg():\n    def __init__(self, max_iter=1000, eta=0.01, lamb = 1):\n        self.eta = eta\n        self.max_iter = max_iter\n        self.lamb = lamb\n        self.w_ = None\n        self.coef_ = None\n        \n    def _stochastic_step(self, w: np.array, X: np.array, y: np.array) -> np.array:\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        #модифицируйте функцию из предыдущей задачи\n        pass\n        \n    def fit(self, X: np.array, y: np.array) -> np.array:\n        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n        n, m = X.shape\n        self.w_ = np.zeros(shape=(m, 1))\n        \n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        \n        self.coef_ = self.w_.reshape(-1)\n        return self\n    \n    def predict(self, X: np.array) -> np.array:\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass\n    \n    def predict_proba(self, X: np.array) -> np.array:\n        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n        pass"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import mean_absolute_error\n######################################################\nX_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\ny_clf = np.array([1, 1, 0, 0])\nmodel = SGDLogReg().fit(X_clf, y_clf)\n\nassert model.lamb == 1\n\nassert_equal(model.predict(np.array([[-0.5, -0.5]])), np.array([0.]))\nassert_equal(model.predict(np.array([[ 0.5,  0.5]])), np.array([1.]))\n######################################################\nnp.random.seed(1337)\nn = 200\na = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\nb = np.random.normal(loc=4, scale=2, size=(n, 2)) #второй класс\nX = np.vstack([a, b]) #двумерный количественный признак\ny = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n\nmodel = SGDLogReg(lamb=0.01).fit(X_train, y_train)\nmodel_real = LogisticRegression(C=0.01, solver='sag').fit(X_train, y_train)\n\nassert model.lamb == 0.01\n\nassert mean_absolute_error(model.predict(X_test), model_real.predict(X_test)) < 0.1\nassert mean_absolute_error(model.predict_proba(X_test), model_real.predict_proba(X_test)) < 0.2\n######################################################\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Momentum"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n\nВ общем случае он будет выглядеть следующим образом: \n\n$$ \\boldsymbol{g}^{(t)} = \\gamma \\boldsymbol{g}^{(t - 1)} + \\eta \\nabla_{\\boldsymbol{w}}L_{k}$$\n$$ \\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\boldsymbol{g}^{(t)}$$\n\nгде\n\n - $\\eta$ — learning rate\n - $\\boldsymbol{w}$ — вектор параметров\n - $\\boldsymbol{g}$ — вектор градиентов \n - $L$ — оптимизируемый функционал\n - $\\gamma$ — momentum term (обычно выбирается 0.9)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.base import BaseEstimator, ClassifierMixin\n\nclass SGDMomentum(BaseEstimator, ClassifierMixin):\n    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000):\n        self.gradient = gradient #функция находящая градиент за вас\n        self.lr = lr\n        self.l = l\n        self.gamma = gamma\n        self.max_iter = max_iter\n        self.w = np.random.normal(size=(features_size + 1, 1))\n\n    def fit(self, X, y):\n        v = np.zeros(self.w.shape)\n        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n        for i in range(self.max_iter):\n            index = np.random.randint(X.shape[0])\n            cur_grad = self.gradient(self.w, X[index, :], np.array(y[index]), self.l)\n            # пересчитайте веса в стохаистическом градиентном спуске\n            '''\n            .∧＿∧ \n            ( ･ω･｡)つ━☆・*。 \n            ⊂  ノ    ・゜+. \n            しーＪ   °。+ *´¨) \n                    .· ´¸.·*´¨) \n                    (¸.·´ (¸.·'* ☆  <YOUR CODE>\n            '''\n            self.w = \n        return self"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["######################################################\nX = np.array([[ 2.67973871e-01, -9.43407158e-01,  4.59566449e-01,\n                1.63136986e-01, -5.44506820e-01]])\ny = np.array([-1])\n\nr0 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=1)\nr0.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\nr0.fit(X, y)\n\nassert np.allclose(r0.w.reshape(6), np.array([ 0.01753165,  0.0350633,   0.05259494,  0.0350633,   0.01753165, -0.01753165]))\n######################################################\n"], "execution_count": null, "outputs": []}], "metadata": {"colab": {"collapsed_sections": [], "name": "features.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 4}