{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_equal\n",
    "from typing import Tuple\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Матричные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подаются вектор $\\boldsymbol{x}$ длины $n$, $\\boldsymbol{c}$ длины $m$ и матрица $A$ размера $\\{n\\times m\\}$\n",
    "\n",
    "Пусть $y = \\boldsymbol{x}^TA\\boldsymbol{c}$\n",
    "\n",
    "Найдите $\\frac{dy}{d\\boldsymbol{x}}$, $\\frac{dy}{dA}$ и $\\frac{dy}{d\\boldsymbol{c}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_deriv(x: np.array, A: np.array, c: np.array):\n",
    "    ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "x = np.array([ [1], [2], [3],  [4]])\n",
    "c = np.array([[25], [5],[-9]])\n",
    "A = np.arange(12).reshape(4,3)\n",
    "\n",
    "y1, y2, y3 = matrix_deriv(x, A, c)\n",
    "\n",
    "assert_array_equal(y1, np.array([[-13],\n",
    "                                [ 50],\n",
    "                                [113],\n",
    "                                [176]]))\n",
    "\n",
    "assert_array_equal(y2, np.array([[ 25,   5,  -9],\n",
    "                                [ 50,  10, -18],\n",
    "                                [ 75,  15, -27],\n",
    "                                [100,  20, -36]]))\n",
    "\n",
    "assert_array_equal(y3, np.array([[60, 70, 80]]))\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Честная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае линейной регрессии по функционалу $MSE$ задачу оптимизации можно записать следующим образом:\n",
    "\n",
    "$$ \\frac{1}{n}\\sum_{k=1}^N (\\boldsymbol{x}_k^T\\boldsymbol{w} - y_k) ^ 2 \\to \\min_{\\boldsymbol{w}}$$\n",
    "\n",
    "Мы уже знаем, что решение будет выглядеть следующим образом:\n",
    "\n",
    "$$\\boldsymbol{w} = \\begin{cases} X^{-1}\\boldsymbol{y}, & n = m\\\\\n",
    "(X^TX)^{-1}X^T\\boldsymbol{y}, & n > m\\\\\n",
    "X^{T}(XX^{T})^{-1}\\boldsymbol{y}, & n < m\n",
    "\\end{cases}$$\n",
    "\n",
    "где $n$ - количество объектов, а $m$ - количество признаков + 1 (дополнительно 1 вес без признака).\n",
    "\n",
    "Решите задачу регрессии честно, используя формулу выше. \n",
    "\n",
    "Чтобы вы не запутались мы сразу написали добавление единичного столбца к матрице $X$.\n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1]])\n",
    "y_train = np.array([2])\n",
    "X_test = np.array([[0.], [2.], [3.]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([1., 3., 4.])\n",
    "model.w_ == np.array([[1.], [1.]])\n",
    "model.coef_ == np.array([1., 1.])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueLinReg():\n",
    "    def __init__(self):\n",
    "        self.w_ = None  # столбец\n",
    "        self.coef_ = None # строка\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1) # добавляем вес без признака\n",
    "\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "\n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        return (X @ self.w_).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "assert_array_almost_equal(model.predict(np.array([[0], [3], [4]])), np.array([0, 3, 4]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[1.], [0.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=2)\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2], [3]])\n",
    "y_reg = np.array([1, -2, 1])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[0],[4]])), np.array([0., 0.]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[0.], [0.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([0., 0.]), decimal=2)\n",
    "######################################################\n",
    "X_reg = np.array([[1]])\n",
    "y_reg = np.array([2])\n",
    "\n",
    "model = TrueLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[0.], [2.], [3.]])), np.array([1., 3., 4.]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[1.], [1.]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 1.]), decimal=2)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=10, n_features=15, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model_my = TrueLinReg().fit(X_reg, y_reg)\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 20\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Честная регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавьте регуляризацию с коэффициентом $\\lambda$ и решите задачу регрессии (для любого соотношения $m$ и $n$ используйте формулу)\n",
    "$$\\boldsymbol{w} = (X^TX + \\lambda E)^{-1}X^T\\boldsymbol{y}$$\n",
    "где $E$ - единичная матрица.\n",
    "\n",
    "Не забудьте сами добавить **справа** единичный столбец к матрице $X$ аналогично предыдущей задаче.\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "lamb = 1\n",
    "\n",
    "X_test = np.array([[0], [4]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "model.predict(X_test) == np.array([0.33, 3])\n",
    "model.w_ == np.array([[0.67], [0.33]])\n",
    "model.coef_ == np.array([0.67, 0.33])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueReg():\n",
    "    def __init__(self, lamb):\n",
    "        self.lamb = lamb\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = TrueReg(1).fit(X_reg, y_reg)\n",
    "\n",
    "assert_array_almost_equal(model.predict(np.array([[0], [4]])), np.array([0.33, 3]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.w_, np.array([[0.67], [0.33]]), decimal=2)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([0.67, 0.33]), decimal=2)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=100, n_features=99, n_targets=1)\n",
    "\n",
    "model = Ridge(1).fit(X_reg, y_reg)\n",
    "model_my = TrueReg(1).fit(X_reg, y_reg)\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "\n",
    "assert_array_almost_equal(model_my.coef_, coef_real, decimal=0)\n",
    "assert sklearn.metrics.mean_absolute_error(model_my.coef_, coef_real) < 1\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход подаются обучающая выборка $(X,\\boldsymbol{y})$ и как-то определенные веса $w$. Верните градиент функции $MSE$ для изменения весов в алгоритме градиентного спуска.\n",
    "$$\\nabla_{\\boldsymbol{w}} L = \\frac{2}{n} X^{T}(X\\boldsymbol{w} - \\boldsymbol{y})$$\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице, считаем, что они уже есть.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[0.2], [0.2]]) # столбец!\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]]) # столбец!\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[-8.4], \n",
    "       [-8.4]]) # возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "w = np.array([[0.2], \n",
    "              [0.2]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[2],[3]])\n",
    "assert_array_almost_equal( gradient_step(w, X, y), np.array([[-5.6], [-5.6]]))\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стохастический градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте стохаистический градиентный спуск: пересчитайте $\\nabla_{\\boldsymbol{w}}L$ только для одного случайно выбранного элемента из выборки $X$.\n",
    "\n",
    "$$L_{k} = (\\boldsymbol{x}_k^T \\boldsymbol{w} - y_k) ^ 2 \\to \\min_{\\boldsymbol{w}}$$\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{w}}L_{k} = \\boldsymbol{x}_k (\\boldsymbol{x}_k^T \\boldsymbol{w}  - y_k)$$\n",
    "где $\\boldsymbol{x}_i$ - вектор объекта выборки, выбранный случайно.  \n",
    "\n",
    "\n",
    "Здесь не нужно добавлять единицы сбоку к матрице $X$, считаем, что они уже есть.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[1], [1], [1]]) #столбец!\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "y = np.array([[1], [0]]) #столбец!\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[0.],\n",
    "       [0.],\n",
    "       [0.]]) #столбец!\n",
    "или\n",
    "array([[2.],\n",
    "       [2.],\n",
    "       [2.]]) #столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stochastic_step(w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "w = np.array([[1], [1], [1]])\n",
    "y = np.array([[1], [0]])\n",
    "X = np.array([[1, 1, 1], [0, 0, 0]])\n",
    "\n",
    "z , k = 0 , 0\n",
    "for i in range(100):\n",
    "    g = stochastic_step(w, X, y)\n",
    "    if g[0] == 2:\n",
    "        z += 1\n",
    "    else:\n",
    "        k += 1\n",
    "assert z >= 35 and k >= 35\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем Линейную регрессию на градиентном спуске. Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса инициализированы нулями. \n",
    "$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta\\nabla_{\\boldsymbol{w}} L$$\n",
    "\n",
    "Используйте параметры `max_iter`=$1000$, `eta`=$0.1$\n",
    "\n",
    "Все итерации проходить не нужно. Остановитесь в момент, когда норма разницы между старыми и новыми весами станет меньше $1e-9$. \n",
    "\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1], [2]])\n",
    "y_train = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg(max_iter=1000, eta=0.1).fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[3],[4]]))\n",
    "\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([3., 4.])\n",
    "model.coef_ = np.array([1., 0.])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDLinReg():\n",
    "    def __init__(self, max_iter=1000, eta=0.1):\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "        self.coef_ = None #строка\n",
    "        self.w_ = None #столбец\n",
    "    \n",
    "    def _gradient_descending(w, X, y):\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros((m, 1)) #столбец\n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)   \n",
    "        return (X @ self.w_).reshape(-1) #возвращаем всегда строку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.datasets import make_regression\n",
    "######################################################\n",
    "X_reg = np.array([[1], [2]])\n",
    "y_reg = np.array([1, 2])\n",
    "\n",
    "model = GDLinReg().fit(X_reg, y_reg)\n",
    "assert_array_almost_equal(model.predict(np.array([[3],[4]])), np.array([3., 4.]), decimal=1)\n",
    "\n",
    "assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=1)\n",
    "######################################################\n",
    "X_reg, y_reg = make_regression(n_samples=200, n_features=10, n_targets=1)\n",
    "\n",
    "model = LinearRegression().fit(X_reg, y_reg)\n",
    "model2 = GDLinReg().fit(X_reg, y_reg)\n",
    "\n",
    "coef_real = np.hstack([model.coef_, model.intercept_])\n",
    "coef_my = model2.coef_\n",
    "\n",
    "assert mean_absolute_error(coef_my, coef_real) < 5\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте L2-регуляризацию (так же известную как Ridge).\n",
    "\n",
    "$$L = \\frac{1}{2}\\lVert X\\boldsymbol{w} - \\boldsymbol{y}\\rVert_2^2 + \\frac{\\lambda}{2}\\lVert \\boldsymbol{w} \\rVert_2^2$$\n",
    "\n",
    "Найдите новый $\\nabla_{\\boldsymbol{w}} L$ и верните его.\n",
    "\n",
    "Обратите внимание, что свободный коэффициент весов (самый последний) **не нужно** регуляризовывать.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "np.array([[28.], \n",
    "          [27.]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step_l2(w: np.array, X: np.array, y: np.array, lamb: float) -> np.array:\n",
    "    # Коэффициент регуляризации lamb\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "w = np.array([[1], [1]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "assert_array_almost_equal(gradient_step_l2(w, X, y, 1), np.array([[28.], [27.]]))\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистический градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь переходим к задаче классификации и Логистической регрессии. Мы уже знаем, что в нашем случае нужно минимизировать метрику $LogLoss$:\n",
    "$$\\ln{L} = -\\sum_{k=1}^{n}y_i\\ln{(\\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + (1 - y_k)\\ln{(1 - \\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))}$$\n",
    "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$\n",
    "\n",
    "Ваша задача взять **производную** этой функции и вернуть вектор градиентов $\\nabla_{\\boldsymbol{w}}\\ln{L}$. Формулу необходимо вывести в векторном виде, чтобы считалось быстрее.\n",
    "\n",
    "Обратите внимание: \n",
    "\n",
    "* $w$, $y$ - столбцы, а не строки. Ответ тоже столбец.\n",
    "* В функции используется **натуральный** логарифм.\n",
    "* Нужно посчитать полный градиент, а не стохастический.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "w = np.array([0, 0])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "y = np.array([[1],[0],[0]])\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "array([[2], \n",
    "       [2]]) #возвращаем столбец!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log_gradient_step(w: np.array, X: np.array, y: np.array)-> np.array:\n",
    "    '''\n",
    "        .∧＿∧ \n",
    "        ( ･ω･｡)つ━☆・*。 \n",
    "        ⊂  ノ    ・゜+. \n",
    "        しーＪ   °。+ *´¨) \n",
    "                .· ´¸.·*´¨) \n",
    "                (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "    '''\n",
    "    #нужно вернуть столбец\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "w = np.array([[0], [0]])\n",
    "X = np.array([[1,1], [2,2], [3,3]])\n",
    "\n",
    "y = np.array([[1],[0],[0]])\n",
    "assert_array_almost_equal(log_gradient_step(w, X, y), np.array([[2], [2]]))\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем Логистическую регрессию на стохастическом градиентном спуске.\n",
    "$$L = -\\ln{Likelihood} = -\\sum_{k=1}^{n}y_k\\ln{(\\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + (1 - y_k)\\ln{(1 - \\sigma(\\boldsymbol{x}_k^{T}\\boldsymbol{w}))} + \\frac{\\lambda}{2}\\lVert \\boldsymbol{w} \\rVert_2^2 \\rightarrow \\min$$\n",
    "где $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, $y \\in \\{0,1\\}$, $\\lVert \\boldsymbol{w} \\rVert_2^2 = \\sum_{i=1}^{m} w_i^2$= - квадрат эвклидовой нормы\n",
    "\n",
    "Собственно вероятность принадлежности определенному классу:\n",
    "$$P(y_k=1) = \\sigma(\\boldsymbol{x}_k^T \\boldsymbol{w})~~~P(y_k=0) = 1 - \\sigma(\\boldsymbol{x}_k^T \\boldsymbol{w})$$\n",
    "\n",
    "Реализуйте пересчёт весов в цикле для алгоритма градиентного спуска. Начальные веса сгенерированны рандомно. \n",
    "\n",
    "$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta\\nabla_{\\boldsymbol{w}} L_k$$\n",
    "\n",
    "Будьте внимательны:\n",
    "\n",
    "* На вход алгоритму приходит $\\boldsymbol{y}$ - **строка**, как и в любой sklearn алгоритм.\n",
    "* Не устанавливайте количество итераций больше 1000, так как алгоритм будет долго работать.\n",
    "* Как и в линейной регрессии не забудьте добавить единичный столбец справа.\n",
    "\n",
    "Можете использовать и обычный спуск, главное чтоб по времени зашло.\n",
    "\n",
    "### Sample 1\n",
    "#### Input:\n",
    "```python\n",
    "X_train = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_train = np.array([1, 1, 0, 0])\n",
    "\n",
    "model = SGDLogReg().fit(X_train, y_train)\n",
    "y_pred = model.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]]))\n",
    "y_prob = model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n",
    "```\n",
    "#### Output:\n",
    "```python\n",
    "y_pred = np.array([1., 0.])\n",
    "y_prob = np.array([[0.1, 0.9],  # это не точный ответ, но очень приблизительный, отличие на 0.1 - это нормально\n",
    "                   [0.9, 0.1]]) # для каждого объекта возвращаем его вероятность нуля и единицы\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGDLogReg():\n",
    "    def __init__(self, max_iter=1000, eta=0.01, lamb = 1):\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.lamb = lamb\n",
    "        self.w_ = None\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def _stochastic_step(self, w: np.array, X: np.array, y: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        #модифицируйте функцию из предыдущей задачи\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X: np.array, y: np.array) -> np.array:\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        n, m = X.shape\n",
    "        self.w_ = np.zeros(shape=(m, 1))\n",
    "        \n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        \n",
    "        self.coef_ = self.w_.reshape(-1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass\n",
    "    \n",
    "    def predict_proba(self, X: np.array) -> np.array:\n",
    "        ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "######################################################\n",
    "X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n",
    "y_clf = np.array([1, 1, 0, 0])\n",
    "model = SGDLogReg().fit(X_clf, y_clf)\n",
    "\n",
    "assert model.lamb == 1\n",
    "\n",
    "assert_equal(model.predict(np.array([[-0.5, -0.5]])), np.array([0.]))\n",
    "assert_equal(model.predict(np.array([[ 0.5,  0.5]])), np.array([1.]))\n",
    "######################################################\n",
    "np.random.seed(1337)\n",
    "n = 200\n",
    "a = np.random.normal(loc=0, scale=1, size=(n, 2)) #первый класс\n",
    "b = np.random.normal(loc=4, scale=2, size=(n, 2)) #второй класс\n",
    "X = np.vstack([a, b]) #двумерный количественный признак\n",
    "y = np.hstack([np.zeros(n), np.ones(n)]) #бинарный признак\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n",
    "\n",
    "model = SGDLogReg(lamb=0.01).fit(X_train, y_train)\n",
    "model_real = LogisticRegression(C=0.01, solver='sag').fit(X_train, y_train)\n",
    "\n",
    "assert model.lamb == 0.01\n",
    "\n",
    "assert mean_absolute_error(model.predict(X_test), model_real.predict(X_test)) < 0.1\n",
    "assert mean_absolute_error(model.predict_proba(X_test), model_real.predict_proba(X_test)) < 0.2\n",
    "######################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ \\boldsymbol{g}^{(t)} = \\gamma \\boldsymbol{g}^{(t - 1)} + \\eta \\nabla_{\\boldsymbol{w}}L_{k}$$\n",
    "$$ \\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\boldsymbol{g}^{(t)}$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\eta$ — learning rate\n",
    " - $\\boldsymbol{w}$ — вектор параметров\n",
    " - $\\boldsymbol{g}$ — вектор градиентов \n",
    " - $L$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SGDMomentum(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, features_size, gradient, lr=0.01, l=1, gamma=0.9, max_iter=1000):\n",
    "        self.gradient = gradient #функция находящая градиент за вас\n",
    "        self.lr = lr\n",
    "        self.l = l\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.w = np.random.normal(size=(features_size + 1, 1))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        v = np.zeros(self.w.shape)\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "        for i in range(self.max_iter):\n",
    "            index = np.random.randint(X.shape[0])\n",
    "            cur_grad = self.gradient(self.w, X[index, :], np.array(y[index]), self.l)\n",
    "            # пересчитайте веса в стохаистическом градиентном спуске\n",
    "            '''\n",
    "            .∧＿∧ \n",
    "            ( ･ω･｡)つ━☆・*。 \n",
    "            ⊂  ノ    ・゜+. \n",
    "            しーＪ   °。+ *´¨) \n",
    "                    .· ´¸.·*´¨) \n",
    "                    (¸.·´ (¸.·'* ☆  <YOUR CODE>\n",
    "            '''\n",
    "            self.w = \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "X = np.array([[ 2.67973871e-01, -9.43407158e-01,  4.59566449e-01,\n",
    "                1.63136986e-01, -5.44506820e-01]])\n",
    "y = np.array([-1])\n",
    "\n",
    "r0 = SGDMomentum(5, lambda a, b, c, d: a, max_iter=10, l=1, lr=1)\n",
    "r0.w = np.array([[0.1], [0.2], [0.3], [0.2], [0.1], [-0.1]])\n",
    "r0.fit(X, y)\n",
    "\n",
    "assert np.allclose(r0.w.reshape(6), np.array([ 0.01753165,  0.0350633,   0.05259494,  0.0350633,   0.01753165, -0.01753165]))\n",
    "######################################################\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
